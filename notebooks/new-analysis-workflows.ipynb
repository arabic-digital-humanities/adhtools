{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nlppln import WorkflowGenerator\n",
    "#cwl_working_dir = '/home/dafne/cwl-working-dir/'\n",
    "cwl_working_dir = '/home/jvdzwaan/cwl-working-dir/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with WorkflowGenerator(working_dir=cwl_working_dir) as wf:\n",
    "    wf.load(steps_dir='../adhtools/cwl/')\n",
    "    wf.load(steps_dir='../java/cwl/')\n",
    "    print(wf.list_steps())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove openiti metadata and divide a file in books/chapters \n",
    "\n",
    "with WorkflowGenerator(working_dir=cwl_working_dir) as wf:\n",
    "    wf.load(steps_dir='../adhtools/cwl/')\n",
    "    wf.load(steps_dir='../java/cwl/')\n",
    "    #print(wf.list_steps())\n",
    "    \n",
    "    txt_file = wf.add_input(txt_file='File')\n",
    "    \n",
    "    doc = \"\"\"Split a text in OpenITI format in smaller pieces.\n",
    "    \n",
    "    First, the OpenITI metadata is removed. Next, the file is split on\n",
    "    OpenITI markers, to be able to retain information about headers and\n",
    "    quotes. Finally, the files are split based on file size, to make\n",
    "    sure SAFAR does not crash on big input files.\n",
    "    \n",
    "    Input:\n",
    "        txt_file (File): The name of the input file, a text in OpenITI format.\n",
    "        \n",
    "    Output:\n",
    "        A list of text files, that can be analyzed or stemmed using SAFAR.\n",
    "    \"\"\"\n",
    "    wf.set_documentation(doc)\n",
    "    \n",
    "    txt_file = wf.openiti2txt(in_file=txt_file)\n",
    "    chapters = wf.split_text_openiti_markers(in_file=txt_file)\n",
    "    snippets = wf.split_text_size(in_file=chapters, scatter='in_file', scatter_method='dotproduct')\n",
    "    \n",
    "    out_files = wf.flatten_list(list=snippets)\n",
    "    \n",
    "    wf.add_outputs(out_files=out_files)\n",
    "    \n",
    "    wf.save('../adhtools/cwl/split-file-chapters.cwl', wd=True, relative=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# split books/chapters for a directory of text files\n",
    "with WorkflowGenerator(working_dir=cwl_working_dir) as wf:\n",
    "    wf.load(steps_dir='../adhtools/cwl/')\n",
    "    wf.load(steps_dir='../java/cwl/')\n",
    "    #print(wf.list_steps())\n",
    "    \n",
    "    in_dir = wf.add_input(in_dir='Directory')\n",
    "    \n",
    "    doc = \"\"\"Call split-file-chapters.cwl for a Directory of files.\n",
    "    \n",
    "    Scattered version of split-file-chapters.cwl.\n",
    "    \n",
    "    Input:\n",
    "        in_dir (Directory): The directory containing texts to be processed.\n",
    "        \n",
    "    Output:\n",
    "        A list (of lists) of text files, that can be analyzed or stemmed using SAFAR.\n",
    "    \"\"\"\n",
    "    wf.set_documentation(doc)\n",
    "    \n",
    "    txt_files = wf.ls(in_dir=in_dir)\n",
    "    chapters = wf.split_file_chapters(txt_file=txt_files,\n",
    "                                      scatter='txt_file', scatter_method='dotproduct')\n",
    "    \n",
    "    wf.add_outputs(texts=chapters)\n",
    "    \n",
    "    wf.save('../adhtools/cwl/split-dir-chapters.cwl', wd=True, relative=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# analyze file (first split it into multiple smaller subfiles based on regexes)\n",
    "# Obsolete? (uses splitting on regexes and does not retain information about headers and quotes)\n",
    "with WorkflowGenerator(working_dir=cwl_working_dir) as wf:\n",
    "    wf.load(steps_dir='../adhtools/cwl/')\n",
    "    wf.load(steps_dir='../java/cwl/')\n",
    "    \n",
    "    analyzer = wf.add_input(analyzer='enum', symbols=['Alkhalil', 'BAMA'], default='Alkhalil')\n",
    "    txt_file = wf.add_input(txt_file='File')\n",
    "    metadata = wf.add_input(metadata='File')\n",
    "    cp = wf.add_input(cp='string')\n",
    "    split_regex_small = wf.add_input(split_regex_small='string[]', default=['Milestone300', '### |', '### ||'])\n",
    "        \n",
    "    txt_file = wf.openiti2txt(in_file=txt_file)\n",
    "    snippets = wf.split_text(in_file=txt_file, regex=split_regex_small)\n",
    "        \n",
    "    analyzed_files = wf.SafarAnalyze(in_files=snippets, analyzer=analyzer, cp=cp)\n",
    "    merged_file = wf.merge_safar_xml(in_files=analyzed_files)\n",
    "    #filtered_file = wf.safar_filter_analyses(in_file=merged_file)\n",
    "    \n",
    "    out_file = wf.safar_add_metadata_file(in_file=merged_file, in_file_meta=metadata)\n",
    "    \n",
    "    # Output is one xml file\n",
    "    wf.add_outputs(out_file=out_file)\n",
    "    \n",
    "    wf.save('../adhtools/cwl/safar-split-and-analyze-file-no-filtering.cwl', wd=True, relative=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# analyze file (first split it into multiple smaller subfiles based on file size)\n",
    "# Obsolete? (does not retain information about headers and quotes)\n",
    "with WorkflowGenerator(working_dir=cwl_working_dir) as wf:\n",
    "    wf.load(steps_dir='../adhtools/cwl/')\n",
    "    wf.load(steps_dir='../java/cwl/')\n",
    "    \n",
    "    analyzer = wf.add_input(analyzer='enum', symbols=['Alkhalil', 'BAMA'], default='Alkhalil')\n",
    "    txt_file = wf.add_input(txt_file='File')\n",
    "    metadata = wf.add_input(metadata='File')\n",
    "    cp = wf.add_input(cp='string')\n",
    "    size = wf.add_input(size='int?')\n",
    "\n",
    "    txt_file = wf.openiti2txt(in_file=txt_file)\n",
    "    snippets = wf.split_text_size(in_file=txt_file, size=size)\n",
    "        \n",
    "    analyzed_files = wf.SafarAnalyze(in_files=snippets, analyzer=analyzer, cp=cp)\n",
    "    merged_file = wf.merge_safar_xml(in_files=analyzed_files)\n",
    "    filtered_file = wf.safar_filter_analyses(in_file=merged_file)\n",
    "    \n",
    "    out_file = wf.safar_add_metadata_file(in_file=filtered_file, in_file_meta=metadata)\n",
    "    \n",
    "    # Output is one xml file\n",
    "    wf.add_outputs(out_file=out_file)\n",
    "    \n",
    "    wf.save('../adhtools/cwl/safar-split-and-analyze-file.cwl', wd=True, relative=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# analyze file \n",
    "# - remove openiti metadata\n",
    "# - divide in chapters (separate headings)\n",
    "# - split based on file size\n",
    "# - analyze\n",
    "# - merge xml files\n",
    "# - filter analyses\n",
    "# - add metadata\n",
    "# Result: an xml file for a book\n",
    "with WorkflowGenerator(working_dir=cwl_working_dir) as wf:\n",
    "    wf.load(steps_dir='../adhtools/cwl/')\n",
    "    wf.load(steps_dir='../java/cwl/')\n",
    "    \n",
    "    analyzer = wf.add_input(analyzer='enum', symbols=['Alkhalil', 'BAMA'], default='Alkhalil')\n",
    "    txt_file = wf.add_input(txt_file='File')\n",
    "    metadata = wf.add_input(metadata='File')\n",
    "    cp = wf.add_input(cp='string')\n",
    "    size = wf.add_input(size='int?')\n",
    "    \n",
    "    doc = \"\"\"Analyze a text file in OpenITI format using SAFAR.\n",
    "    \n",
    "    To be able to retain information about headers and quotes, the text is first \n",
    "    split into files for text, headers and quotes. Next, the resulting files are\n",
    "    split based on file size, because SAFAR crashes if the output XML files become\n",
    "    too large. Next, the small files are analyzed using SAFAR and the resulting XML\n",
    "    files are merged into one big file, containing metadata and information about \n",
    "    which words are part of headers and quotes. Finally, to reduce the size of the \n",
    "    output XML, redundant information is filtered out.\n",
    "    \n",
    "    Inputs:\n",
    "        analyzer (enum): The SAFAR analyzer to use. Options are (Alkhalil, BAMA).\n",
    "        txt_file (File): The name of the file to analyze, should be in a text in \n",
    "            OpenITI format.\n",
    "        metadata (File): The name of the csv file containing the corpus metadata.\n",
    "        cp (string): The class path including where the SAFAR binaries can be found.\n",
    "        size (int): Maximum file size in bytes. The text is spilt on the first \n",
    "            space after the desired file size is reached. So the file size slightly \n",
    "            differs between files.\n",
    "    \n",
    "    Output:\n",
    "        File in SAFAR analyzer output, containing metadata and information about headers\n",
    "            and quotes.\n",
    "    \"\"\"\n",
    "    wf.set_documentation(doc)\n",
    "        \n",
    "    txt_files = wf.split_file_chapters(txt_file=txt_file)\n",
    "        \n",
    "    analyzed_files = wf.SafarAnalyze(in_files=txt_files, analyzer=analyzer, cp=cp)\n",
    "    merged_file = wf.merge_safar_xml(in_files=analyzed_files)\n",
    "    filtered_file = wf.safar_filter_analyses(in_file=merged_file)\n",
    "    \n",
    "    out_file = wf.safar_add_metadata_file(in_file=filtered_file, in_file_meta=metadata)\n",
    "    \n",
    "    # Output is one xml file\n",
    "    wf.add_outputs(out_file=out_file)\n",
    "    \n",
    "    wf.save('../adhtools/cwl/safar-split-and-analyze-file.cwl', wd=True, relative=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# analyze file \n",
    "# - remove openiti metadata\n",
    "# - divide in chapters (separate headings)\n",
    "# - split based on file size\n",
    "# - analyze\n",
    "# - merge xml files\n",
    "# - filter analyses\n",
    "# - add metadata\n",
    "# Result: an xml file for a book\n",
    "with WorkflowGenerator(working_dir=cwl_working_dir) as wf:\n",
    "    wf.load(steps_dir='../adhtools/cwl/')\n",
    "    wf.load(steps_dir='../java/cwl/')\n",
    "    \n",
    "    analyzer = wf.add_input(analyzer='enum', symbols=['Alkhalil', 'BAMA'], default='Alkhalil')\n",
    "    txt_file = wf.add_input(txt_file='File')\n",
    "    #metadata = wf.add_input(metadata='File')\n",
    "    cp = wf.add_input(cp='string')\n",
    "    size = wf.add_input(size='int?')\n",
    "        \n",
    "    txt_files = wf.split_file_chapters(txt_file=txt_file)\n",
    "        \n",
    "    analyzed_files = wf.SafarAnalyze(in_files=txt_files, analyzer=analyzer, cp=cp)\n",
    "    merged_file = wf.merge_safar_xml(in_files=analyzed_files)\n",
    "    filtered_file = wf.safar_filter_analyses(in_file=merged_file)\n",
    "    \n",
    "    #out_file = wf.safar_add_metadata_file(in_file=filtered_file, in_file_meta=metadata)\n",
    "    \n",
    "    # Output is one xml file\n",
    "    wf.add_outputs(out_file=filtered_file)\n",
    "    \n",
    "    wf.save('../adhtools/cwl/safar-split-and-analyze-file-no-merge-metadata.cwl', wd=True, relative=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split and analyze multiple books\n",
    "with WorkflowGenerator(working_dir=cwl_working_dir) as wf:\n",
    "    wf.load(steps_dir='../adhtools/cwl/')\n",
    "    wf.load(steps_dir='../java/cwl/')\n",
    "    \n",
    "    analyzer = wf.add_input(analyzer='enum', symbols=['Alkhalil', 'BAMA'], default='Alkhalil')\n",
    "    in_dir = wf.add_input(in_dir='Directory')\n",
    "    metadata = wf.add_input(metadata='File')\n",
    "    cp = wf.add_input(cp='string')\n",
    "    size = wf.add_input(size='int?')\n",
    "    \n",
    "    doc = \"\"\"Analyze a directory of text files in OpenITI format using SAFAR.\n",
    "    \n",
    "    Calls `safar-split-and-analyze-file.cwl` for each file in the input directory.\n",
    "    \n",
    "    Inputs:\n",
    "        analyzer (enum): The SAFAR analyzer to use. Options are (Alkhalil, BAMA).\n",
    "        in_dir (Directory): Directory containing files to analyze.\n",
    "        metadata (File): The name of the csv file containing the corpus metadata.\n",
    "        cp (string): The class path including where the SAFAR binaries can be found.\n",
    "        size (int): Maximum file size in bytes. The text is spilt on the first \n",
    "            space after the desired file size is reached. So the file size slightly \n",
    "            differs between files.\n",
    "    \n",
    "    Output:\n",
    "        A list of files in SAFAR analyzer XML. There is an output file for each file \n",
    "            in the input directory.\n",
    "    \"\"\"\n",
    "    wf.set_documentation(doc)\n",
    "    \n",
    "    books = wf.ls(in_dir=in_dir)\n",
    "    \n",
    "    safar_output = wf.safar_split_and_analyze_file(analyzer=analyzer, txt_file=books, cp=cp, \n",
    "                                                   size=size, metadata=metadata,\n",
    "                                                   scatter='txt_file', scatter_method='dotproduct')\n",
    "    \n",
    "    wf.add_outputs(safar_output=safar_output)\n",
    "    \n",
    "    wf.save('../adhtools/cwl/safar-split-and-analyze-dir.cwl', wd=True, relative=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split and analyze multiple books\n",
    "with WorkflowGenerator(working_dir=cwl_working_dir) as wf:\n",
    "    wf.load(steps_dir='../adhtools/cwl/')\n",
    "    wf.load(steps_dir='../java/cwl/')\n",
    "    \n",
    "    analyzer = wf.add_input(analyzer='enum', symbols=['Alkhalil', 'BAMA'], default='Alkhalil')\n",
    "    in_dir = wf.add_input(in_dir='Directory')\n",
    "    #metadata = wf.add_input(metadata='File')\n",
    "    cp = wf.add_input(cp='string')\n",
    "    size = wf.add_input(size='int?')  \n",
    "    \n",
    "    books = wf.ls(in_dir=in_dir)\n",
    "    \n",
    "    safar_output = wf.safar_split_and_analyze_file_no_merge_metadata(analyzer=analyzer, txt_file=books, cp=cp, \n",
    "                                                   size=size, \n",
    "                                                   scatter='txt_file', scatter_method='dotproduct')\n",
    "    \n",
    "    wf.add_outputs(safar_output=safar_output)\n",
    "    \n",
    "    wf.save('../adhtools/cwl/safar-split-and-analyze-dir-no-merge-metadata.cwl', wd=True, relative=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# split and stem file file \n",
    "# - remove openiti metadata\n",
    "# - divide in chapters (separate headings)\n",
    "# - split based on file size\n",
    "# - analyze\n",
    "# - merge xml files\n",
    "# (- filter analyses)\n",
    "# - add metadata\n",
    "# Result: an xml file for a book\n",
    "with WorkflowGenerator(working_dir=cwl_working_dir) as wf:\n",
    "    wf.load(steps_dir='../adhtools/cwl/')\n",
    "    wf.load(steps_dir='../java/cwl/')\n",
    "    \n",
    "    stemmer = wf.add_input(stemmer='enum', \n",
    "                           symbols=['KHOJA', 'LIGHT10', 'ISRI', 'MOTAZ', 'TASHAPHYNE'], \n",
    "                           default='LIGHT10')\n",
    "    txt_file = wf.add_input(txt_file='File')\n",
    "    metadata = wf.add_input(metadata='File')\n",
    "    cp = wf.add_input(cp='string')\n",
    "    \n",
    "    doc = \"\"\"Stem a text file in OpenITI format using SAFAR.\n",
    "    \n",
    "    To be able to retain information about headers and quotes, the text is first \n",
    "    split into files for text, headers and quotes. Next, the resulting files are\n",
    "    split based on file size, because SAFAR crashes if the output XML files become\n",
    "    too large. Next, the small files are stemmed using SAFAR and the resulting XML\n",
    "    files are merged into one big file, containing metadata and information about \n",
    "    which words are part of headers and quotes.\n",
    "    \n",
    "    Inputs:\n",
    "        stemmer (enum): The SAFAR stemmer to use. Options are (KHOJA, LIGHT10, \n",
    "            ISRI, MOTAZ, TASHAPHYNE).\n",
    "        txt_file (File): The name of the file to stem, should be in a text in \n",
    "            OpenITI format.\n",
    "        metadata (File): The name of the csv file containing the corpus metadata.\n",
    "        cp (string): The class path including where the SAFAR binaries can be found.\n",
    "    \n",
    "    Output:\n",
    "        File in SAFAR stemmer output, containing metadata and information about headers\n",
    "            and quotes.\n",
    "    \"\"\"\n",
    "    wf.set_documentation(doc)\n",
    "        \n",
    "    txt_files = wf.split_file_chapters(txt_file=txt_file)\n",
    "        \n",
    "    stemmed_files = wf.SafarStem(in_files=txt_files, stemmer=stemmer, cp=cp)\n",
    "    merged_file = wf.merge_safar_xml(in_files=stemmed_files)\n",
    "    filtered_file = merged_file #wf.safar_filter_analyses(in_file=merged_file) # No need to filter stemmed, because there for stemming there is always a single analysis.\n",
    "    \n",
    "    out_file = wf.safar_add_metadata_file(in_file=filtered_file, in_file_meta=metadata)\n",
    "    \n",
    "    # Output is one xml file\n",
    "    wf.add_outputs(out_file=out_file)\n",
    "    \n",
    "    wf.save('../adhtools/cwl/safar-split-and-stem-file.cwl', wd=True, relative=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split and stem a directory containing text files\n",
    "with WorkflowGenerator(working_dir=cwl_working_dir) as wf:\n",
    "    wf.load(steps_dir='../adhtools/cwl/')\n",
    "    wf.load(steps_dir='../java/cwl/')\n",
    "    \n",
    "    stemmer = wf.add_input(stemmer='enum', \n",
    "                           symbols=['KHOJA', 'LIGHT10', 'ISRI', 'MOTAZ', 'TASHAPHYNE'], \n",
    "                           default='LIGHT10')\n",
    "    in_dir = wf.add_input(in_dir='Directory')\n",
    "    metadata = wf.add_input(metadata='File')\n",
    "    cp = wf.add_input(cp='string')\n",
    "    \n",
    "    doc = \"\"\"Analyze a directory of text files in OpenITI format using SAFAR.\n",
    "    \n",
    "    Calls `safar-split-and-stem-file.cwl` for each file in the input directory.\n",
    "       \n",
    "    Inputs:\n",
    "        stemmer (enum): The SAFAR stemmer to use. Options are (KHOJA, LIGHT10, \n",
    "            ISRI, MOTAZ, TASHAPHYNE).\n",
    "        in_dir (Directory): Directory containing files to analyze.\n",
    "        metadata (File): The name of the csv file containing the corpus metadata.\n",
    "        cp (string): The class path including where the SAFAR binaries can be found.\n",
    "    \n",
    "    Output:\n",
    "        A list of files in SAFAR stemmer XML. There is an output file for each file \n",
    "            in the input directory.\n",
    "    \"\"\"\n",
    "    wf.set_documentation(doc)\n",
    "    \n",
    "    txt_files = wf.ls(in_dir=in_dir)\n",
    "    out_files = wf.safar_split_and_stem_file(txt_file=txt_files, metadata=metadata, stemmer=stemmer, cp=cp,\n",
    "                                             scatter='txt_file', scatter_method='dotproduct')\n",
    "\n",
    "    wf.add_outputs(out_files=out_files)\n",
    "    \n",
    "    wf.save('../adhtools/cwl/safar-split-and-stem-dir.cwl', wd=True, relative=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with WorkflowGenerator(working_dir=cwl_working_dir) as wf:\n",
    "    wf.load(steps_dir='../adhtools/cwl/')\n",
    "    wf.load(steps_dir='../java/cwl/')\n",
    "    \n",
    "    in_dir = wf.add_input(in_dir='Directory')\n",
    "    metadata = wf.add_input(metadata='File')\n",
    "    \n",
    "    doc = \"\"\"Add metadata to all xml files in a directory\n",
    "    \n",
    "    Calls `safar-split-and-stem-file.cwl` for each file in the input directory.\n",
    "       \n",
    "    Inputs:\n",
    "        in_dir (Directory): The directory containing the input files.\n",
    "        metadata (File): The name of the csv file containing the corpus metadata.\n",
    "    \n",
    "    Output:\n",
    "        A list of xml files with metadata. There is an output file for each file \n",
    "            in the input directory.\n",
    "    \"\"\"\n",
    "    wf.set_documentation(doc)\n",
    "    \n",
    "    in_files = wf.ls(in_dir=in_dir)\n",
    "    out_files = wf.safar_add_metadata_file(in_file=in_files, in_file_meta=metadata,\n",
    "                                          scatter='in_file', scatter_method='dotproduct')\n",
    "    \n",
    "    wf.add_outputs(out_files=out_files)\n",
    "    wf.save('../adhtools/cwl/add-metadata-dir.cwl', wd=True, relative=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python",
   "pygments_lexer": "ipython3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
