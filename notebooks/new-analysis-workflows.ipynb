{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nlppln import WorkflowGenerator\n",
    "cwl_working_dir = '/home/dafne/cwl-working-dir/'\n",
    "#cwl_working_dir = '/home/jvdzwaan/cwl-working-dir/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/dafne/anaconda2/envs/adh/lib/python3.6/site-packages/jupyter_client/jsonutil.py:67: DeprecationWarning: Interpreting naive datetime as local 2018-10-11 11:02:31.072061. Please add timezone info to timestamps.\n",
      "  new_obj[k] = extract_dates(v)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Steps\n",
      "  SafarAnalyze............. out_files = wf.SafarAnalyze(cp, in_files[, analyzer, xmx])\n",
      "  SafarStem................ out_files = wf.SafarStem(cp, in_files[, stemmer])\n",
      "  align.................... changes, metadata = wf.align(file1, file2[, out_dir])\n",
      "  apachetika............... out_files = wf.apachetika(in_files[, tika_server])\n",
      "  archive2dir.............. out_dir = wf.archive2dir(archive[, remove-dir-structure])\n",
      "  basic-text-statistics.... metadata_out = wf.basic_text_statistics(in_files, out_file)\n",
      "  check-utf8............... utf8_files = wf.check_utf8(in_dir[, convert])\n",
      "  clear-xml-elements....... out_file = wf.clear_xml_elements(element, xml_file)\n",
      "  copy-and-rename.......... copy = wf.copy_and_rename(in_file[, rename])\n",
      "  create-chunked-list...... chunks = wf.create_chunked_list(in_dir[, out_name])\n",
      "  delete-empty-files....... out_files = wf.delete_empty_files(in_dir)\n",
      "  extract_metadata......... out_meta, out_txt = wf.extract_metadata(in_file)\n",
      "  filter-nes............... filtered_nerstats = wf.filter_nes(nerstats[, name])\n",
      "  flatten-dirs............. out = wf.flatten_dirs(in_dirs[, dir_name])\n",
      "  flatten-list............. out_files = wf.flatten_list(list)\n",
      "  freqs.................... freqs = wf.freqs(in_files[, name])\n",
      "  frog-dir................. frogout = wf.frog_dir(in_dir[, skip])\n",
      "  frog-single-text......... frogout = wf.frog_single_text(in_file)\n",
      "  frog-to-saf.............. saf = wf.frog_to_saf(in_files)\n",
      "  gather-dirs.............. out = wf.gather_dirs(in_dirs[, dir_name])\n",
      "  ixa-pipe-tok............. out_file = wf.ixa_pipe_tok(language, in_file)\n",
      "  liwc-tokenized........... liwc = wf.liwc_tokenized(in_dir, liwc_dict[, encoding])\n",
      "  lowercase................ out_files = wf.lowercase(in_file)\n",
      "  ls....................... out_files = wf.ls(in_dir[, endswith, recursive])\n",
      "  ls_chunk................. out_files = wf.ls_chunk(chunks, in_dir[, name])\n",
      "  merge-csv................ merged = wf.merge_csv(in_files[, name])\n",
      "  merge-safar-xml.......... out_file = wf.merge_safar_xml(in_files)\n",
      "  mkdir.................... out = wf.mkdir(dir_name)\n",
      "  normalize-whitespace-punctuation metadata_out = wf.normalize_whitespace_punctuation(meta_in)\n",
      "  openiti2txt.............. out_file = wf.openiti2txt(in_file)\n",
      "  pattern.................. saf = wf.pattern(in_file[, language])\n",
      "  prettify-xml............. out_file = wf.prettify_xml(in_file)\n",
      "  remove-newlines.......... out_files = wf.remove_newlines(in_file[, replacement])\n",
      "  remove-xml-elements...... out_file = wf.remove_xml_elements(element, xml_file)\n",
      "  replace-ner.............. out_files = wf.replace_ner(metadata, in_files[, mode])\n",
      "  saf-to-freqs............. freqs = wf.saf_to_freqs(in_files[, mode, name])\n",
      "  saf-to-txt............... out_files = wf.saf_to_txt(in_files)\n",
      "  safar-add-metadata-file.. out_file = wf.safar_add_metadata_file(in_file, in_file_meta)\n",
      "  safar-add-metadata....... out_dir = wf.safar_add_metadata(in_dir_meta, in_file_meta, in_files)\n",
      "  safar-filter-analyses.... out_file = wf.safar_filter_analyses(in_file)\n",
      "  save-dir-to-subdir....... out = wf.save_dir_to_subdir(inner_dir, outer_dir)\n",
      "  save-files-to-dir........ out = wf.save_files_to_dir(in_files[, dir_name])\n",
      "  save-ner-data............ ner_statistics = wf.save_ner_data(in_files[, name])\n",
      "  split-text-openiti-markers out_files = wf.split_text_openiti_markers(in_file)\n",
      "  split-text-size.......... out_files = wf.split_text_size(in_file[, size])\n",
      "  split-text............... out_files = wf.split_text(in_file, regex)\n",
      "  tar-compress............. tar_out = wf.tar_compress(tarfile, in_dir)\n",
      "  tar...................... out = wf.tar(in_file)\n",
      "  textDNA-generate......... json = wf.textDNA_generate(in_dir, mode[, folder_sequences, name_prefix, output_dir])\n",
      "  txt2safar-input.......... out_dir, out_dir_meta, out_file_meta = wf.txt2safar_input(in_file)\n",
      "  untar.................... out_files = wf.untar(tarfile)\n",
      "  xml-to-text.............. out_files = wf.xml_to_text(in_files[, tag])\n",
      "  zip-dir-flat............. zip_file = wf.zip_dir_flat(in_dir[, zip_name])\n",
      "\n",
      "Workflows\n",
      "  add-metadata-dir......... out_files = wf.add_metadata_dir(in_dir, metadata)\n",
      "  analyze-and-index-dir-all-analyzers indexed, xml = wf.analyze_and_index_dir_all_analyzers(cp, in_dir[, analyzer, index_dir_name, xml_dir_name])\n",
      "  analyze-and-index-dir.... indexed, merged_dir = wf.analyze_and_index_dir(cp, in_dir[, action, analyzer, content_viewable, index_format, index_name, text_direction, xml_dir_name])\n",
      "  anonymize................ ner_stats, txt = wf.anonymize(txt_dir)\n",
      "  safar-analyze-archive.... result = wf.safar_analyze_archive(archive[, analyzer, cp, index_name])\n",
      "  safar-analyze-book....... safar_output_dir = wf.safar_analyze_book(book, cp[, analyzer])\n",
      "  safar-analyze-corpus..... safar_output = wf.safar_analyze_corpus(cp, in_dir[, analyzer, index_name])\n",
      "  safar-analyze-dir........ out_files = wf.safar_analyze_dir(cp, in_dir[, analyzer, recursive])\n",
      "  safar-split-and-analyze-dir safar_output = wf.safar_split_and_analyze_dir(cp, in_dir, metadata[, analyzer, size])\n",
      "  safar-split-and-analyze-file out_file = wf.safar_split_and_analyze_file(cp, metadata, txt_file[, analyzer, size])\n",
      "  safar-split-and-analyze-file-no-filtering out_file = wf.safar_split_and_analyze_file_no_filtering(cp, metadata, txt_file[, analyzer, split_regex_small])\n",
      "  safar-split-and-stem-dir. out_files = wf.safar_split_and_stem_dir(cp, in_dir, metadata[, regex, stemmer])\n",
      "  safar-split-and-stem-file out_file = wf.safar_split_and_stem_file(cp, in_file, metadata[, regex, stemmer])\n",
      "  safar-stem-book.......... safar_output_dir = wf.safar_stem_book(book, cp[, stemmer])\n",
      "  safar-stem-dir........... safar_output = wf.safar_stem_dir(cp, in_dir[, stemmer])\n",
      "  safar-workflow........... safar_output_dir = wf.safar_workflow(book, cp[, analyzer])\n",
      "  split-dir-chapters....... texts = wf.split_dir_chapters(in_dir)\n",
      "  split-file-chapters...... out_files = wf.split_file_chapters(txt_file)\n",
      "  untar-tar................ tar_out = wf.untar_tar(out_name, tarfile)\n"
     ]
    }
   ],
   "source": [
    "with WorkflowGenerator(working_dir=cwl_working_dir) as wf:\n",
    "    wf.load(steps_dir='../adhtools/cwl/')\n",
    "    wf.load(steps_dir='../java/cwl/')\n",
    "    print(wf.list_steps())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/dafne/anaconda2/envs/adh/lib/python3.6/site-packages/jupyter_client/jsonutil.py:67: DeprecationWarning: Interpreting naive datetime as local 2018-10-09 11:55:47.449271. Please add timezone info to timestamps.\n",
      "  new_obj[k] = extract_dates(v)\n"
     ]
    }
   ],
   "source": [
    "# remove openiti metadata and divide a file in books/chapters \n",
    "\n",
    "with WorkflowGenerator(working_dir=cwl_working_dir) as wf:\n",
    "    wf.load(steps_dir='../adhtools/cwl/')\n",
    "    wf.load(steps_dir='../java/cwl/')\n",
    "    #print(wf.list_steps())\n",
    "    \n",
    "    txt_file = wf.add_input(txt_file='File')\n",
    "    \n",
    "    txt_file = wf.openiti2txt(in_file=txt_file)\n",
    "    chapters = wf.split_text_openiti_markers(in_file=txt_file)\n",
    "    snippets = wf.split_text_size(in_file=chapters, scatter='in_file', scatter_method='dotproduct')\n",
    "    \n",
    "    out_files = wf.flatten_list(list=snippets)\n",
    "    \n",
    "    wf.add_outputs(out_files=out_files)\n",
    "    \n",
    "    wf.save('../adhtools/cwl/split-file-chapters.cwl', wd=True, relative=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/dafne/anaconda2/envs/adh/lib/python3.6/site-packages/jupyter_client/jsonutil.py:67: DeprecationWarning: Interpreting naive datetime as local 2018-10-09 11:55:55.779504. Please add timezone info to timestamps.\n",
      "  new_obj[k] = extract_dates(v)\n"
     ]
    }
   ],
   "source": [
    "# split books/chapters for a directory of text files\n",
    "with WorkflowGenerator(working_dir=cwl_working_dir) as wf:\n",
    "    wf.load(steps_dir='../adhtools/cwl/')\n",
    "    wf.load(steps_dir='../java/cwl/')\n",
    "    #print(wf.list_steps())\n",
    "    \n",
    "    in_dir = wf.add_input(in_dir='Directory')\n",
    "    \n",
    "    txt_files = wf.ls(in_dir=in_dir)\n",
    "    chapters = wf.split_file_chapters(txt_file=txt_files,\n",
    "                                      scatter='txt_file', scatter_method='dotproduct')\n",
    "    \n",
    "    wf.add_outputs(texts=chapters)\n",
    "    \n",
    "    wf.save('../adhtools/cwl/split-dir-chapters.cwl', wd=True, relative=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/dafne/anaconda2/envs/adh/lib/python3.6/site-packages/jupyter_client/jsonutil.py:67: DeprecationWarning: Interpreting naive datetime as local 2018-10-09 11:55:58.711092. Please add timezone info to timestamps.\n",
      "  new_obj[k] = extract_dates(v)\n"
     ]
    }
   ],
   "source": [
    "# analyze file (first split it into multiple smaller subfiles based on regexes)\n",
    "with WorkflowGenerator(working_dir=cwl_working_dir) as wf:\n",
    "    wf.load(steps_dir='../adhtools/cwl/')\n",
    "    wf.load(steps_dir='../java/cwl/')\n",
    "    \n",
    "    analyzer = wf.add_input(analyzer='enum', symbols=['Alkhalil', 'BAMA'], default='Alkhalil')\n",
    "    txt_file = wf.add_input(txt_file='File')\n",
    "    metadata = wf.add_input(metadata='File')\n",
    "    cp = wf.add_input(cp='string')\n",
    "    split_regex_small = wf.add_input(split_regex_small='string[]', default=['Milestone300', '### |', '### ||'])\n",
    "        \n",
    "    txt_file = wf.openiti2txt(in_file=txt_file)\n",
    "    snippets = wf.split_text(in_file=txt_file, regex=split_regex_small)\n",
    "        \n",
    "    analyzed_files = wf.SafarAnalyze(in_files=snippets, analyzer=analyzer, cp=cp)\n",
    "    merged_file = wf.merge_safar_xml(in_files=analyzed_files)\n",
    "    #filtered_file = wf.safar_filter_analyses(in_file=merged_file)\n",
    "    \n",
    "    out_file = wf.safar_add_metadata_file(in_file=merged_file, in_file_meta=metadata)\n",
    "    \n",
    "    # Output is one xml file\n",
    "    wf.add_outputs(out_file=out_file)\n",
    "    \n",
    "    wf.save('../adhtools/cwl/safar-split-and-analyze-file-no-filtering.cwl', wd=True, relative=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/dafne/anaconda2/envs/adh/lib/python3.6/site-packages/jupyter_client/jsonutil.py:67: DeprecationWarning: Interpreting naive datetime as local 2018-10-09 11:56:01.920081. Please add timezone info to timestamps.\n",
      "  new_obj[k] = extract_dates(v)\n"
     ]
    }
   ],
   "source": [
    "# analyze file (first split it into multiple smaller subfiles based on file size)\n",
    "with WorkflowGenerator(working_dir=cwl_working_dir) as wf:\n",
    "    wf.load(steps_dir='../adhtools/cwl/')\n",
    "    wf.load(steps_dir='../java/cwl/')\n",
    "    \n",
    "    analyzer = wf.add_input(analyzer='enum', symbols=['Alkhalil', 'BAMA'], default='Alkhalil')\n",
    "    txt_file = wf.add_input(txt_file='File')\n",
    "    metadata = wf.add_input(metadata='File')\n",
    "    cp = wf.add_input(cp='string')\n",
    "    size = wf.add_input(size='int?')\n",
    "        \n",
    "    txt_file = wf.openiti2txt(in_file=txt_file)\n",
    "    snippets = wf.split_text_size(in_file=txt_file, size=size)\n",
    "        \n",
    "    analyzed_files = wf.SafarAnalyze(in_files=snippets, analyzer=analyzer, cp=cp)\n",
    "    merged_file = wf.merge_safar_xml(in_files=analyzed_files)\n",
    "    filtered_file = wf.safar_filter_analyses(in_file=merged_file)\n",
    "    \n",
    "    out_file = wf.safar_add_metadata_file(in_file=filtered_file, in_file_meta=metadata)\n",
    "    \n",
    "    # Output is one xml file\n",
    "    wf.add_outputs(out_file=out_file)\n",
    "    \n",
    "    wf.save('../adhtools/cwl/safar-split-and-analyze-file.cwl', wd=True, relative=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/dafne/anaconda2/envs/adh/lib/python3.6/site-packages/jupyter_client/jsonutil.py:67: DeprecationWarning: Interpreting naive datetime as local 2018-10-09 11:56:08.708033. Please add timezone info to timestamps.\n",
      "  new_obj[k] = extract_dates(v)\n"
     ]
    }
   ],
   "source": [
    "# analyze file \n",
    "# - remove openiti metadata\n",
    "# - divide in chapters (separate headings)\n",
    "# - split based on file size\n",
    "# - analyze\n",
    "# - merge xml files\n",
    "# - filter analyses\n",
    "# - add metadata\n",
    "# Result: an xml file for a book\n",
    "with WorkflowGenerator(working_dir=cwl_working_dir) as wf:\n",
    "    wf.load(steps_dir='../adhtools/cwl/')\n",
    "    wf.load(steps_dir='../java/cwl/')\n",
    "    \n",
    "    analyzer = wf.add_input(analyzer='enum', symbols=['Alkhalil', 'BAMA'], default='Alkhalil')\n",
    "    txt_file = wf.add_input(txt_file='File')\n",
    "    metadata = wf.add_input(metadata='File')\n",
    "    cp = wf.add_input(cp='string')\n",
    "    size = wf.add_input(size='int?')\n",
    "        \n",
    "    txt_files = wf.split_file_chapters(txt_file=txt_file)\n",
    "        \n",
    "    analyzed_files = wf.SafarAnalyze(in_files=txt_files, analyzer=analyzer, cp=cp)\n",
    "    merged_file = wf.merge_safar_xml(in_files=analyzed_files)\n",
    "    filtered_file = wf.safar_filter_analyses(in_file=merged_file)\n",
    "    \n",
    "    out_file = wf.safar_add_metadata_file(in_file=filtered_file, in_file_meta=metadata)\n",
    "    \n",
    "    # Output is one xml file\n",
    "    wf.add_outputs(out_file=out_file)\n",
    "    \n",
    "    wf.save('../adhtools/cwl/safar-split-and-analyze-file.cwl', wd=True, relative=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/dafne/anaconda2/envs/adh/lib/python3.6/site-packages/jupyter_client/jsonutil.py:67: DeprecationWarning: Interpreting naive datetime as local 2018-10-09 11:57:43.933974. Please add timezone info to timestamps.\n",
      "  new_obj[k] = extract_dates(v)\n"
     ]
    }
   ],
   "source": [
    "# Split and analyze multiple books\n",
    "with WorkflowGenerator(working_dir=cwl_working_dir) as wf:\n",
    "    wf.load(steps_dir='../adhtools/cwl/')\n",
    "    wf.load(steps_dir='../java/cwl/')\n",
    "    \n",
    "    analyzer = wf.add_input(analyzer='enum', symbols=['Alkhalil', 'BAMA'], default='Alkhalil')\n",
    "    in_dir = wf.add_input(in_dir='Directory')\n",
    "    metadata = wf.add_input(metadata='File')\n",
    "    cp = wf.add_input(cp='string')\n",
    "    size = wf.add_input(size='int?')  \n",
    "    \n",
    "    books = wf.ls(in_dir=in_dir)\n",
    "    \n",
    "    safar_output = wf.safar_split_and_analyze_file(analyzer=analyzer, txt_file=books, cp=cp, \n",
    "                                                   size=size, metadata=metadata,\n",
    "                                                   scatter='txt_file', scatter_method='dotproduct')\n",
    "    \n",
    "    wf.add_outputs(safar_output=safar_output)\n",
    "    \n",
    "    wf.save('../adhtools/cwl/safar-split-and-analyze-dir.cwl', wd=True, relative=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/dafne/anaconda2/envs/adh/lib/python3.6/site-packages/jupyter_client/jsonutil.py:67: DeprecationWarning: Interpreting naive datetime as local 2018-10-11 11:03:48.693993. Please add timezone info to timestamps.\n",
      "  new_obj[k] = extract_dates(v)\n"
     ]
    }
   ],
   "source": [
    "# split and stem file file \n",
    "# - remove openiti metadata\n",
    "# - divide in chapters (separate headings)\n",
    "# - split based on file size\n",
    "# - analyze\n",
    "# - merge xml files\n",
    "# (- filter analyses)\n",
    "# - add metadata\n",
    "# Result: an xml file for a book\n",
    "with WorkflowGenerator(working_dir=cwl_working_dir) as wf:\n",
    "    wf.load(steps_dir='../adhtools/cwl/')\n",
    "    wf.load(steps_dir='../java/cwl/')\n",
    "    \n",
    "    stemmer = wf.add_input(stemmer='enum', \n",
    "                           symbols=['KHOJA', 'LIGHT10', 'ISRI', 'MOTAZ', 'TASHAPHYNE'], \n",
    "                           default='LIGHT10')\n",
    "    txt_file = wf.add_input(txt_file='File')\n",
    "    metadata = wf.add_input(metadata='File')\n",
    "    cp = wf.add_input(cp='string')\n",
    "    size = wf.add_input(size='int?')\n",
    "        \n",
    "    txt_files = wf.split_file_chapters(txt_file=txt_file)\n",
    "        \n",
    "    stemmed_files = wf.SafarStem(in_files=txt_files, stemmer=stemmer, cp=cp)\n",
    "    merged_file = wf.merge_safar_xml(in_files=stemmed_files)\n",
    "    filtered_file = merged_file #wf.safar_filter_analyses(in_file=merged_file) # to do: also filter stemmed?\n",
    "    \n",
    "    out_file = wf.safar_add_metadata_file(in_file=filtered_file, in_file_meta=metadata)\n",
    "    \n",
    "    # Output is one xml file\n",
    "    wf.add_outputs(out_file=out_file)\n",
    "    \n",
    "    wf.save('../adhtools/cwl/safar-split-and-stem-file.cwl', wd=True, relative=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/dafne/anaconda2/envs/adh/lib/python3.6/site-packages/jupyter_client/jsonutil.py:67: DeprecationWarning: Interpreting naive datetime as local 2018-10-09 12:03:36.552569. Please add timezone info to timestamps.\n",
      "  new_obj[k] = extract_dates(v)\n"
     ]
    }
   ],
   "source": [
    "# Split and stem single book (txt file)\n",
    "with WorkflowGenerator(working_dir=cwl_working_dir) as wf:\n",
    "    wf.load(steps_dir='../adhtools/cwl/')\n",
    "    wf.load(steps_dir='../java/cwl/')\n",
    "    \n",
    "    stemmer = wf.add_input(stemmer='enum', \n",
    "                           symbols=['KHOJA', 'LIGHT10', 'ISRI', 'MOTAZ', 'TASHAPHYNE'], \n",
    "                           default='LIGHT10')\n",
    "    in_dir = wf.add_input(in_dir='Directory')\n",
    "    metadata = wf.add_input(metadata='File')\n",
    "    regex = wf.add_input(regex='string[]', default=['### |', '### ||'])\n",
    "    cp = wf.add_input(cp='string')\n",
    "    \n",
    "    txt_files = wf.ls(in_dir=in_dir)\n",
    "    out_files = wf.safar_split_and_stem_file(in_file=txt_files, metadata=metadata, stemmer=stemmer, cp=cp,\n",
    "                                             scatter='in_file', scatter_method='dotproduct')\n",
    "\n",
    "    wf.add_outputs(out_files=out_files)\n",
    "    \n",
    "    wf.save('../adhtools/cwl/safar-split-and-stem-dir.cwl', wd=True, relative=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/dafne/anaconda2/envs/adh/lib/python3.6/site-packages/jupyter_client/jsonutil.py:67: DeprecationWarning: Interpreting naive datetime as local 2018-10-09 12:03:37.083704. Please add timezone info to timestamps.\n",
      "  new_obj[k] = extract_dates(v)\n"
     ]
    }
   ],
   "source": [
    "with WorkflowGenerator(working_dir=cwl_working_dir) as wf:\n",
    "    wf.load(steps_dir='../adhtools/cwl/')\n",
    "    wf.load(steps_dir='../java/cwl/')\n",
    "    \n",
    "    in_dir = wf.add_input(in_dir='Directory')\n",
    "    metadata = wf.add_input(metadata='File')\n",
    "    \n",
    "    in_files = wf.ls(in_dir=in_dir)\n",
    "    out_files = wf.safar_add_metadata_file(in_file=in_files, in_file_meta=metadata,\n",
    "                                          scatter='in_file', scatter_method='dotproduct')\n",
    "    \n",
    "    wf.add_outputs(out_files=out_files)\n",
    "    wf.save('../adhtools/cwl/add-metadata-dir.cwl', wd=True, relative=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:adh]",
   "language": "python",
   "name": "conda-env-adh-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
