{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import OrderedDict\n",
    "\n",
    "from lxml import etree\n",
    "from tqdm import tqdm_notebook as tqdm\n",
    "\n",
    "import re\n",
    "\n",
    "\n",
    "def read_book(fname, book_id):\n",
    "    result = OrderedDict()\n",
    "    regex = '_{9,}\\n'\n",
    "\n",
    "    # Extract the words\n",
    "    context = etree.iterparse(fname, events=('end', ), tag=('book'))\n",
    "    for event, elem in context:\n",
    "        for e in elem.iterchildren():\n",
    "            if e.tag == 'id':\n",
    "                t_id = e.text\n",
    "                #print(t_id)\n",
    "            elif e.tag == 'nass':   # text\n",
    "                # sometimes the text is None (e.g., file 16, id 214)\n",
    "                text = e.text\n",
    "                fn = None\n",
    "                if text is None:\n",
    "                    print('None text found in {} (prev? id: {})'.format(fname, t_id))\n",
    "                else:\n",
    "                    # &#xd; is automatically replaced by \\r, but we would like to have \\n\n",
    "                    text = text.replace('\\r', '\\n')\n",
    "                    \n",
    "                    # get the footnotes if they are there\n",
    "                    parts = re.split(regex, text)\n",
    "                    \n",
    "                    # we shouln't have parts longer than 2\n",
    "                    if len(parts) > 2:                        \n",
    "                        # all exceptions\n",
    "                        print('book_id', book_id)\n",
    "                        if book_id in (27, 38):\n",
    "                            text = parts[0]\n",
    "                            fn = regex.join(parts[1:])\n",
    "                        elif book_id in (36, 70, 89, 124, 140, 142, 143, 146, 147, 148):\n",
    "                            # seems to be a table of contents and/or index\n",
    "                            # so, put the content in the notes\n",
    "                            text = ' '\n",
    "                            fn = text\n",
    "                        else:\n",
    "                            pass\n",
    "                            print('More parts found in {} ({})'.format(t_id, len(parts)))\n",
    "                            #print(text)\n",
    "                            #print('---')\n",
    "    \n",
    "                    if len(parts) == 2:\n",
    "                        t, fn = parts\n",
    "                        text = t\n",
    "\n",
    "            elif e.tag == 'page':\n",
    "                page = e.text\n",
    "            elif e.tag == 'part':\n",
    "                part = e.text\n",
    "            #print(e.tag)\n",
    "        \n",
    "        # ignore entries that don't have text\n",
    "        if not text is None:\n",
    "            result[t_id] = {'text': text, 'page': page, 'part': part}\n",
    "        \n",
    "        if not fn is None:\n",
    "            result[t_id]['footnotes'] = fn\n",
    "\n",
    "        # make iteration over context fast and consume less memory\n",
    "        #https://www.ibm.com/developerworks/xml/library/x-hiperfparse\n",
    "        elem.clear()\n",
    "        while elem.getprevious() is not None:\n",
    "            del elem.getparent()[0]\n",
    "    \n",
    "    return result\n",
    "\n",
    "book = read_book('/home/jvdzwaan/data/adh-corpora/dawa/New_xml_corpus/217/book.xml', 1)\n",
    "print(len(book))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_title(fname):\n",
    "    result = {}\n",
    "    levels = {}\n",
    "    \n",
    "    # Extract the words\n",
    "    context = etree.iterparse(fname, events=('end', ), tag=('title'))\n",
    "    for event, elem in context:\n",
    "        for e in elem.iterchildren():\n",
    "            if e.tag == 'id':\n",
    "                t_id = e.text\n",
    "                #print(t_id)\n",
    "            elif e.tag == 'tit':\n",
    "                title = e.text\n",
    "                # &#xd; is automatically replaced by \\r, but we would like to have \\n\n",
    "                title = title.replace('\\r', '\\n')\n",
    "            elif e.tag == 'lvl':\n",
    "                level = e.text\n",
    "                levels[level] = None\n",
    "            #print(e.tag)\n",
    "        if not t_id in result.keys():\n",
    "            result[t_id] = []\n",
    "            \n",
    "        result[t_id].append({'title': title, 'level': level})\n",
    "\n",
    "        # make iteration over context fast and consume less memory\n",
    "        #https://www.ibm.com/developerworks/xml/library/x-hiperfparse\n",
    "        elem.clear()\n",
    "        while elem.getprevious() is not None:\n",
    "            del elem.getparent()[0]\n",
    "    \n",
    "    #print('levels found:', levels.keys())\n",
    "    return result, levels.keys()\n",
    "\n",
    "title, levels = read_title('/home/jvdzwaan/data/adh-corpora/dawa/New_xml_corpus/217/title.xml')\n",
    "print(len(title))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize_arabic(text):\n",
    "    # Remove non-arabic characters\n",
    "    nonarab_chars = '[^\\u0621-\\u064A ]'\n",
    "    text = re.sub(nonarab_chars, '', text)\n",
    "    text = text.strip()\n",
    "    text = re.sub(\"[إأٱآا]\", \"ا\", text)\n",
    "    text = re.sub(\"ى\", \"ي\", text)\n",
    "    text = re.sub(\"ة\", \"ه\", text)\n",
    "    return text\n",
    "\n",
    "def preprocess_arabic(text):\n",
    "    # Remove non-arabic characters\n",
    "    nonarab_chars = '[^\\u0621-\\u064A ]'\n",
    "    text = re.sub(nonarab_chars, '', text)\n",
    "    text = text.strip()\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# combine book and title\n",
    "# and write to file\n",
    "import re\n",
    "import codecs\n",
    "\n",
    "from fuzzywuzzy import fuzz, process\n",
    "\n",
    "def replace_in_lines(text, title, lvl):\n",
    "    #print('text in')\n",
    "    #print(text)\n",
    "    #print('---')\n",
    "    replaced = False\n",
    "    lines = text.split('\\n')\n",
    "    for i, line in enumerate(lines):\n",
    "        if title == line:\n",
    "            lines[i] = line.replace(line, '### {} {}\\n'.format(int(lvl)*'|', line))\n",
    "            replaced=True\n",
    "            #print('found exact match')\n",
    "            break\n",
    "    if not replaced:\n",
    "        for i, line in enumerate(lines):\n",
    "            if title in line and not line.startswith('#'):\n",
    "                lines[i] = line.replace(line, '### {} {}\\n'.format(int(lvl)*'|', line))\n",
    "                \n",
    "    text = '\\n'.join(lines)\n",
    "    #print('text out')\n",
    "    #print(text)\n",
    "    #print('---')\n",
    "    return text\n",
    "\n",
    "\n",
    "def combine_data(book, title, out_file):\n",
    "    start = 0\n",
    "    in_text = 0\n",
    "    not_found = 0\n",
    "    norm_tit = 0\n",
    "    with codecs.open(out_file, 'w', encoding='utf-8') as f:\n",
    "        for b_id, data in book.items():\n",
    "            text = data['text']\n",
    "            num = b_id\n",
    "            #num = str(int(b_id)+1)\n",
    "            if num in title:\n",
    "                #print(b_id)\n",
    "                #print('title!')\n",
    "                #print(title[b_id])\n",
    "                # Sometimes the titles in the xml are in the wrong order\n",
    "                # So sort them to put them in the right order\n",
    "                title[num].sort(key=lambda t: int(t['level']))\n",
    "                #print(title[b_id])\n",
    "                #print('---')\n",
    "                for t in title[num]:\n",
    "                    ti = t['title']\n",
    "                    lvl = t['level']\n",
    "                    replaced = False\n",
    "                    #print(title[b_id]['title'], title[b_id]['level'])\n",
    "                    #f.write('### {} {}\\n'.format(int(lvl)*'|', ti))\n",
    "                    if text.strip().startswith(ti):\n",
    "                        #print('title at start of text', b_id)\n",
    "                        text = text.replace(ti, '### {} {}\\n'.format(int(lvl)*'|', ti), 1)\n",
    "                        start += 1\n",
    "                        replaced = True\n",
    "                    elif ti in text:\n",
    "                        #print('title in text', b_id)\n",
    "                        # find line with text\n",
    "                        text = replace_in_lines(text, ti, lvl)\n",
    "                        in_text += 1\n",
    "                        replaced = True\n",
    "                    else:\n",
    "                        #text = '### {} {}\\n'.format(int(lvl)*'|', ti) + text\n",
    "                        #print('title text not found', b_id)\n",
    "                        title_normalized = normalize_arabic(ti)\n",
    "                        if title_normalized in text:\n",
    "                            #print('Normalized title found!')\n",
    "                            norm_tit += 1\n",
    "                            text = replace_in_lines(text, title_normalized, lvl)\n",
    "                            \n",
    "                        else:\n",
    "                            lines = text.split('\\n')\n",
    "                            print('Title not found', b_id)\n",
    "                            print('<id>{}</id>'.format(b_id))\n",
    "                            not_found += 1\n",
    "                            \n",
    "                            # For these titles, the fuzzy matching does not work properly, so don't do it\n",
    "                            if b_id not in (12, 17, 25, 32, 33, 34, 35, 36, 37, 38, 43, 44, 47, 48, 49, \n",
    "                                            51, 52, 53, 54, 57, 59, 60, 61, 62, 65, 68, 70, 72, 73, 75, \n",
    "                                            76, 77, 78, 79, 80, 81, 85, 90, 91, 92, 93, 94, 95, 96, 97, \n",
    "                                            98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, \n",
    "                                            110, 111, 112, 113, 114, 115, 116, 117, 118, 119, 120, 121, \n",
    "                                            122, 123, 124, 125, 127, 128, 129, 130, 131, 134, 135, 136, \n",
    "                                            137, 138, 139, 141, 142, 143, 144, 145, 146, 147, 148, 149, \n",
    "                                            150, 151, 152, 154, 155, 156, 162, 164, 167, 170, 176, 181, \n",
    "                                            188, 189, 190, 191, 192, 193, 194, 195, 198, 199, 201, 202, \n",
    "                                            203, 204, 205, 206, 207, 208, 210, 211, 212, 214, 217, 218, \n",
    "                                            219, 229, 245, 246, 250, 251, 252, 253, 254, 255, 256, 259, \n",
    "                                            263, 264, 271, 273, 275, 278, 281, 283):\n",
    "                                #print(len(lines), 'lines found')\n",
    "                                match = process.extractOne(title_normalized, lines)\n",
    "                                print('fuzzy match', match[1])\n",
    "                                print(match[0])\n",
    "                                #print('text')\n",
    "                                #print(text)\n",
    "                                #print('---')\n",
    "                            \n",
    "                                text = text.replace(match[0], '### {} {}\\n'.format(int(lvl)*'|', match[0]))\n",
    "                            \n",
    "                                #text_normalized = normalize_arabic(text)\n",
    "                                #print(len(text), len(text_normalized))\n",
    "                                #print(text == text_normalized)\n",
    "                                #if title_normalized in text_normalized:\n",
    "                                #    print('Normalized title found in normalized text!')\n",
    "                                \n",
    "                                # find beginning and end of match\n",
    "                                #regex = r'{}'.format(title_normalized)\n",
    "                                #print(regex)\n",
    "                                #matches = re.finditer(regex, text_normalized)\n",
    "                                # we replace the first occurence of the title \n",
    "                                # but we want to know about the other occurences\n",
    "                                #for i, m in enumerate(matches):\n",
    "                                #    print(i, 'm', m)\n",
    "                                #    if i == 0:\n",
    "                                #        print(m.start())\n",
    "                                #        print(m.end())\n",
    "                                #        prep = preprocess_arabic(text)\n",
    "                                #        extr = prep[m.start()-1:m.end()]\n",
    "                                #        print('extracted:\\t', extr)\n",
    "                                #        print(extr in text)\n",
    "                                #        print(text)\n",
    "                                        #result = []\n",
    "                                        #for c1, c2 in zip(ti, m.group()):\n",
    "                                        #    if c1 == c2:\n",
    "                                        #        result.append(c1)\n",
    "                                        #    else:\n",
    "                                        #        result.append('.')\n",
    "                                        #print('regex:', ''.join(result))\n",
    "                                        #matches = re.finditer(r'{}'.format(regex), text)\n",
    "                                        #for m in matches:\n",
    "                                        #    print(m)\n",
    "                            print('unnormalized:\\t', ti)\n",
    "                            print('normalized:\\t', title_normalized)\n",
    "                            print()\n",
    "                    #print('-----RESULT-----')\n",
    "                    #print(text)\n",
    "                    #print('----')\n",
    "                    \n",
    "                    # Remove the first occurence of the title string\n",
    "                    #text = text.replace(ti, '', 1)\n",
    "            f.write(text.strip())\n",
    "            f.write('\\n')\n",
    "            # sometimes the page marker information is incomplete (e.g. for book 12)\n",
    "            # in that case, we don't write the marker information\n",
    "            if not data['part'] is None and not data['page'] is None:\n",
    "                page = data['page'].split()[0]\n",
    "                part = data['part'].split()[0]\n",
    "                \n",
    "                try:\n",
    "                    part = int(part)\n",
    "                except ValueError:\n",
    "                    print('Invalid part {} ({})'.format(part, out_file))\n",
    "                    part = 0\n",
    "                    \n",
    "                try:\n",
    "                    page = int(page)\n",
    "                except ValueError:\n",
    "                    print('Invalid page {} ({})'.format(page, out_file))\n",
    "                    page = 0\n",
    "                \n",
    "                page_marker = 'V{:03}P{:04}'.format(part, page)\n",
    "                f.write(page_marker)\n",
    "                f.write('\\n')\n",
    "    print('Titles found: {}, in text: {}, normalized: {}, not found: {}'.format(start, in_text, norm_tit, not_found))\n",
    "    #print('---')\n",
    "\n",
    "book = read_book('/home/jvdzwaan/data/adh-corpora/dawa/New_xml_corpus/12/book.xml', 1)\n",
    "title, levels = read_title('/home/jvdzwaan/data/adh-corpora/dawa/New_xml_corpus/12/title.xml')\n",
    "combine_data(book, title, 'test.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "in_dir = '/home/jvdzwaan/data/adh-corpora/dawa/New_xml_corpus/'\n",
    "out_dir = '/home/jvdzwaan/data/adh-corpora/dawa/txt/'\n",
    "notes_dir = '/home/jvdzwaan/data/adh/dawa-notes/'\n",
    "\n",
    "level_data = []\n",
    "\n",
    "for i in tqdm(range(1, 286)):\n",
    "    out_name = '{}.txt'.format(i)\n",
    "    \n",
    "    d = os.path.join(in_dir, str(i))\n",
    "    book_file = os.path.join(d, 'book.xml')\n",
    "    title_file = os.path.join(d, 'title.xml')\n",
    "    \n",
    "    if not os.path.isfile(book_file):\n",
    "        print('Book file \"{}\" not found!'.format(book_file))\n",
    "        \n",
    "    if not os.path.isfile(title_file):\n",
    "        print('Title file \"{}\" not found!'.format(title_file))\n",
    "    \n",
    "    #print(book_file)\n",
    "    #print(title_file)\n",
    "    book = read_book(book_file, i)\n",
    "    title, levels = read_title(title_file)\n",
    "    \n",
    "    level_data.append({'book': i, 'levels': list(levels)})\n",
    "    \n",
    "    # write txt to output file\n",
    "    out_file = os.path.join(out_dir, out_name)\n",
    "    print(out_file)\n",
    "    combine_data(book, title, out_file)\n",
    "    \n",
    "    # write notes, etc. to output file\n",
    "    notes = []\n",
    "    for b_id, data in book.items():\n",
    "        fn = data.get('footnotes')\n",
    "        if fn is not None:\n",
    "            notes.append(fn)\n",
    "    \n",
    "    # save notes to file\n",
    "    if len(notes) > 0:\n",
    "        out_file = os.path.join(notes_dir, out_name)\n",
    "        print('Writing notes to', out_file)\n",
    "        with open(out_file, 'w') as f:\n",
    "            f.write('\\n'.join(notes))\n",
    "            f.write('\\n')\n",
    "                \n",
    "    print('-'*80)\n",
    "    \n",
    "levels = pd.DataFrame(level_data)\n",
    "levels = levels.set_index('book')\n",
    "levels.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "levels.to_csv('dawa_levels.csv', encoding='utf-8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import OrderedDict\n",
    "\n",
    "from lxml import etree\n",
    "from tqdm import tqdm_notebook as tqdm\n",
    "\n",
    "import re\n",
    "\n",
    "\n",
    "def read_almanar(fname, book_id=286):\n",
    "    result = OrderedDict()\n",
    "    regex = '__________\\n'\n",
    "    \n",
    "    # Extract the words\n",
    "    context = etree.iterparse(fname, events=('end', ), tag=('book'))\n",
    "    for event, elem in context:\n",
    "        for e in elem.iterchildren():\n",
    "            if e.tag == 'id':\n",
    "                t_id = e.text\n",
    "                #print(t_id)\n",
    "            elif e.tag == 'nass':   # text\n",
    "                # sometimes the text is None (e.g., file 16, id 214)\n",
    "                text = e.text\n",
    "                fn = None\n",
    "                author = None\n",
    "                if text is None:\n",
    "                    print('None text found in {} (prev? id: {})'.format(fname, t_id))\n",
    "                else:\n",
    "                    # &#xd; is automatically replaced by \\r, but we would like to have \\n\n",
    "                    text = text.replace('\\r', '\\n')\n",
    "                    \n",
    "                    # get the author name and footnotes\n",
    "                    parts = re.split(regex, text)\n",
    "                    \n",
    "                    #print(t_id, len(parts))\n",
    "                    one = parts[0].startswith('الكاتب')\n",
    "                    two = 'الكاتب' in parts[0]\n",
    "                    \n",
    "                    # the author name is always the first word in the part before the line\n",
    "                    if not one and two:\n",
    "                        print('Unexpected in', t_id)\n",
    "                        \n",
    "                    # if there are three parts, the first is always the author name\n",
    "                    # (we assume part[1] is text and part[2] are footnotes)\n",
    "                    if len(parts) == 3: \n",
    "                        if not one:\n",
    "                            print('3 parts found without author name', t_id)\n",
    "                        text = ''.join((parts[0], parts[1]))\n",
    "                        fn = parts[2]\n",
    "                        author = parts[0]\n",
    "                    \n",
    "                    # if there are two parts, and there is no author name in the first,\n",
    "                    # part[0] is text and part[1] are footnotes\n",
    "                    if len(parts) == 2 and not one:\n",
    "                        print('2 parts found without author name', t_id)\n",
    "                        text = parts[0]\n",
    "                        fn = parts[1]\n",
    "                    # with author name, parts[0] is the author name, parts[1] is the text\n",
    "                    elif len(parts) == 2 and one:\n",
    "                        pass\n",
    "                        text = ''.join((parts[0], parts[1]))\n",
    "                        author = parts[0]\n",
    "                        print('2 parts found with author name', t_id)\n",
    "                    \n",
    "                    # if there is one part, the text probably is a title (level 1 or level 2)\n",
    "                    if len(parts) == 1:\n",
    "                        #print('1 part found', t_id)\n",
    "                        if one:\n",
    "                            print('author name in title', t_id)\n",
    "                        text = parts[0]\n",
    "                        \n",
    "                    # author name should be kept in the text and put in the paratext file\n",
    "                    # parts with author name can be recognized by checking whether the text starts with الكاتب\n",
    "\n",
    "            elif e.tag == 'page':\n",
    "                page = e.text\n",
    "            elif e.tag == 'part':\n",
    "                part = e.text\n",
    "            #print(e.tag)\n",
    "        \n",
    "        # ignore entries that don't have text\n",
    "        if not text is None:\n",
    "            result[t_id] = {'text': text, 'page': page, 'part': part}\n",
    "        \n",
    "        if not fn is None:\n",
    "            result[t_id]['footnotes'] = fn\n",
    "            \n",
    "        if not author is None:\n",
    "            result[t_id]['author'] = author\n",
    "\n",
    "        # make iteration over context fast and consume less memory\n",
    "        #https://www.ibm.com/developerworks/xml/library/x-hiperfparse\n",
    "        elem.clear()\n",
    "        while elem.getprevious() is not None:\n",
    "            del elem.getparent()[0]\n",
    "    \n",
    "    return result\n",
    "\n",
    "book = read_book('/home/jvdzwaan/data/adh-corpora/dawa/New_xml_corpus/1/book.xml', 1)\n",
    "print(len(book))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# split file 286 on level 1 (this file contains multiple journals)\n",
    "\n",
    "title_file = '/home/jvdzwaan/data/adh-corpora/dawa/New_xml_corpus/286/title.xml'\n",
    "book_file = '/home/jvdzwaan/data/adh-corpora/dawa/New_xml_corpus/286/book.xml'\n",
    "\n",
    "book = read_almanar(book_file, 286)\n",
    "print(len(book))\n",
    "title, levels = read_title(title_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# combine book and title\n",
    "# and write to file\n",
    "import codecs\n",
    "import os\n",
    "\n",
    "def write_almanar(book, title, start_n, level='1'):\n",
    "    to_write = []\n",
    "    journal = []\n",
    "    notes_year = []\n",
    "    notes = []\n",
    "    first = True\n",
    "    for b_id, data in book.items():\n",
    "        text = data['text']\n",
    "        if b_id in title:\n",
    "            #print(b_id)\n",
    "            #print('title!')\n",
    "            #print(title[b_id])\n",
    "            # Sometimes the titles in the xml are in the wrong order\n",
    "            # So sort them to put them in the right order\n",
    "            title[b_id].sort(key=lambda t: int(t['level']))\n",
    "            #print(title[b_id])\n",
    "            #print('---')\n",
    "            for t in title[b_id]:\n",
    "                ti = t['title']\n",
    "                lvl = t['level']\n",
    "                \n",
    "                if lvl == level:\n",
    "                    if not first:\n",
    "                        to_write.append(''.join(journal))\n",
    "                        notes_year.append(''.join(notes))\n",
    "                    else:\n",
    "                        first = False\n",
    "                    journal = []\n",
    "                    notes = []\n",
    "                    \n",
    "                \n",
    "                #print(title[b_id]['title'], title[b_id]['level'])\n",
    "                journal.append('### {} {}\\n'.format(int(lvl)*'|', ti))\n",
    "                    \n",
    "                # Remove the first occurence of the title string\n",
    "                text = text.replace(ti, '', 1)\n",
    "        journal.append(text.strip())\n",
    "        journal.append('\\n')\n",
    "        \n",
    "        author = data.get('author')\n",
    "        if author is not None:\n",
    "            notes.append(author)\n",
    "        \n",
    "        fn = data.get('footnotes')\n",
    "        if fn is not None:\n",
    "            notes.append(fn)\n",
    "        \n",
    "        # sometimes the page marker information is incomplete (e.g. for book 12)\n",
    "        # in that case, we don't write the marker information\n",
    "        if not data['part'] is None and not data['page'] is None:\n",
    "            page = data['page'].split()[0]\n",
    "            part = data['part'].split()[0]\n",
    "                \n",
    "            try:\n",
    "                part = int(part)\n",
    "            except ValueError:\n",
    "                print('Invalid part {}'.format(part))\n",
    "                part = 0\n",
    "                    \n",
    "            try:\n",
    "                page = int(page)\n",
    "            except ValueError:\n",
    "                print('Invalid page {}'.format(page))\n",
    "                page = 0\n",
    "                \n",
    "            page_marker = 'V{:03}P{:04}'.format(part, page)\n",
    "            journal.append(page_marker)\n",
    "            journal.append('\\n')\n",
    "            #print('---')\n",
    "    print(len(to_write))\n",
    "    \n",
    "    # write text files\n",
    "    for i, text in enumerate(to_write):\n",
    "        n = start_n + i\n",
    "        out_file = '/home/jvdzwaan/data/adh-corpora/dawa/txt/{}.txt'.format(n)\n",
    "        print(out_file)\n",
    "        with open(out_file, 'w') as f:\n",
    "            f.write(text)\n",
    "            f.write('\\n')\n",
    "            \n",
    "    # write notes\n",
    "    for i, ns in enumerate(notes_year):\n",
    "        n = i + start_n\n",
    "        if ns != '':\n",
    "            out_file = os.path.join(notes_dir, '{}.txt'.format(n))\n",
    "            print(out_file)\n",
    "            with open(out_file, 'w') as f:\n",
    "                f.write(ns)\n",
    "                f.write('\\n')\n",
    "        \n",
    "notes_dir = '/home/jvdzwaan/data/adh/dawa-notes/'\n",
    "write_almanar(book, title, 286)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# files 283 and 262 also need to be split on level 1\n",
    "# Can we reuse read-almanar and write-almanar?\n",
    "\n",
    "i = 262\n",
    "start_n = 321\n",
    "\n",
    "title_file = '/home/jvdzwaan/data/adh-corpora/dawa/New_xml_corpus/{}/title.xml'.format(i)\n",
    "book_file = '/home/jvdzwaan/data/adh-corpora/dawa/New_xml_corpus/{}/book.xml'.format(i)\n",
    "\n",
    "book = read_almanar(book_file, i)\n",
    "print(len(book))\n",
    "title, levels = read_title(title_file)\n",
    "    \n",
    "write_almanar(book, title, start_n, level='2')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "i = 283\n",
    "start_n = 336\n",
    "\n",
    "title_file = '/home/jvdzwaan/data/adh-corpora/dawa/New_xml_corpus/{}/title.xml'.format(i)\n",
    "book_file = '/home/jvdzwaan/data/adh-corpora/dawa/New_xml_corpus/{}/book.xml'.format(i)\n",
    "\n",
    "print(title_file)\n",
    "print(book_file)\n",
    "\n",
    "book = read_almanar(book_file, i)\n",
    "print(len(book))\n",
    "title, levels = read_title(title_file)\n",
    "    \n",
    "write_almanar(book, title, start_n)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 278 should be split on level 3 headers\n",
    "        \n",
    "notes_dir = '/home/jvdzwaan/data/adh/dawa-notes/'\n",
    "\n",
    "i = 278\n",
    "start_n = 366\n",
    "\n",
    "title_file = '/home/jvdzwaan/data/adh-corpora/dawa/New_xml_corpus/{}/title.xml'.format(i)\n",
    "book_file = '/home/jvdzwaan/data/adh-corpora/dawa/New_xml_corpus/{}/book.xml'.format(i)\n",
    "\n",
    "print(title_file)\n",
    "print(book_file)\n",
    "\n",
    "book = read_almanar(book_file, i)\n",
    "print(len(book))\n",
    "title, levels = read_title(title_file)\n",
    "write_almanar(book, title, start_n, '3')"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python",
   "pygments_lexer": "ipython3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
