{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import holoviews as hv\n",
    "hv.extension('bokeh', 'matplotlib')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv('/home/jvdzwaan/data/tmp/category-list.csv')\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "values = data['cat'].values.reshape(10,10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import xarray as xr\n",
    "\n",
    "xr_data = xr.DataArray(values, dims=['x', 'y'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make x an y coordinates\n",
    "for ix, iy in np.ndindex(values.shape):\n",
    "    print(ix,iy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds = hv.Dataset((range(10), range(10), values), ['x', 'y'], ['cat'])\n",
    "ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img = hv.Image((range(10), range(10), values), datatype=['grid'])\n",
    "img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%opts Image (cmap='viridis')\n",
    "ds.to(hv.Image, ['x', 'y'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# taken from test2.ipynb\n",
    "from lxml import etree\n",
    "from tqdm import tqdm\n",
    "\n",
    "def stemmer_xml2df2(fname):\n",
    "    result = []\n",
    "    \n",
    "    # Extract the words\n",
    "    context = etree.iterparse(fname, events=('end', ), tag=('word'))\n",
    "    for event, elem in context:\n",
    "        stem = None\n",
    "        for a in elem.getchildren():\n",
    "            if a.tag == 'analysis':\n",
    "                stem = a.attrib['stem']\n",
    "        result.append({'word': elem.attrib['value'], 'proposed_root': stem})\n",
    "        \n",
    "        # make iteration over context fast and consume less memory\n",
    "        #https://www.ibm.com/developerworks/xml/library/x-hiperfparse\n",
    "        elem.clear()\n",
    "        while elem.getprevious() is not None:\n",
    "            del elem.getparent()[0]\n",
    "    \n",
    "    return pd.DataFrame(result)\n",
    "\n",
    "def analyzer_xml2df2(fname):\n",
    "    result = []\n",
    "    \n",
    "    # Extract the words\n",
    "    context = etree.iterparse(fname, events=('end', ), tag=('word'))\n",
    "    for event, elem in tqdm(context):\n",
    "        word = elem.attrib['value']\n",
    "        #print(repr(word))\n",
    "        if word != '':\n",
    "            roots = []\n",
    "            for a in elem.getchildren():\n",
    "                if a.tag == 'analysis':\n",
    "                    try:\n",
    "                        roots.append(a.attrib['root'])\n",
    "                    except:\n",
    "                        pass\n",
    "            roots = list(set(roots))\n",
    "            if len(roots) == 0:\n",
    "                roots.append('NOANALYSIS')\n",
    "            result.append({'word': elem.attrib['value'], 'proposed_root': '\\\\'.join(roots)})\n",
    "        \n",
    "        # make iteration over context fast and consume less memory\n",
    "        #https://www.ibm.com/developerworks/xml/library/x-hiperfparse\n",
    "        elem.clear()\n",
    "        while elem.getprevious() is not None:\n",
    "            del elem.getparent()[0]\n",
    "    \n",
    "    return pd.DataFrame(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "senses_roots = list('''سمع\n",
    "بصر\n",
    "لمس\n",
    "شمم\n",
    "ذوق'''.split('\\n'))\n",
    "# select certain root\n",
    "root = senses_roots[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import OrderedDict\n",
    "\n",
    "query = OrderedDict({'hear': [], 'see': [], 'touch': [], 'smell': [], 'taste': []})\n",
    "query['hear'].append(senses_roots[0])\n",
    "query['see'].append(senses_roots[1])\n",
    "query['touch'].append(senses_roots[2])\n",
    "query['smell'].append(senses_roots[3])\n",
    "query['taste'].append(senses_roots[4])\n",
    "query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fname = '/home/jvdzwaan/data/tmp/adh/analysis/alkhalil/0179MalikIbnAnas.Muwatta.xml'\n",
    "df = analyzer_xml2df2(fname)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "def get_root_cat(row, query):\n",
    "    prop_roots = row['proposed_root'].split('\\\\')\n",
    "    for i, (cat, roots) in enumerate(query.items()):\n",
    "        for root in roots:\n",
    "            if root in prop_roots:\n",
    "                return i+1\n",
    "    return 0\n",
    "\n",
    "df['senses'] = df.apply(lambda row: get_root_cat(row, query), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['senses'].sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "line = 300\n",
    "\n",
    "def pad_df(df, line):\n",
    "    num_to_add = line-(len(df)%line)\n",
    "    \n",
    "    to_add = pd.DataFrame([{'word': 'PAD', 'proposed_root': 'PAD', 'senses': -1} for i in range(num_to_add)])\n",
    "    result = df.append(to_add, sort=False)\n",
    "    return result.reset_index(drop=True)\n",
    "    \n",
    "    \n",
    "df = pad_df(df, line)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(df)%line"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = len(df)//line\n",
    "print(y)\n",
    "values = df['senses'].values.reshape(y, line)\n",
    "print(values.shape)\n",
    "\n",
    "ds = hv.Dataset((list(range(line))[::-1], list(range(y))[::-1], values), ['x', 'y'], ['cat'])\n",
    "ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(list(range(line)[::-1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mosaic = ds.to(hv.Image, ['x', 'y'])\n",
    "\n",
    "mosaic.options(cmap=['#d3d3d3', '#ffffff', '#e6194B', '#4363d8', '#ffe119', '#911eb4', '#3cb44b'], colorbar=True, width=600, height=800)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mosaic = ds.to(hv.Points, kdims=['x', 'y'], vdims=['cat'])\n",
    "mosaic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "frequencies, edges = np.histogram(values[np.where(values >= 1)], [1,2,3,4,5,6])\n",
    "hv.Histogram((edges, frequencies))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "edges"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "values[np.where(values >= 1)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['senses'][df['senses'] >= 1].hist()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The five legal categories - STEMS\n",
    "\n",
    "In THREE texts: Sarakhsi, Ibn Qudama, and Muhaqqiq al-Hilli\n",
    "\n",
    "* forbidden: محظور = light red AND  حرام = dark red\n",
    "* discouraged: مكروه = dark orange AND مذموم = light orange\n",
    "* neutral: مباح = green\n",
    "* recommended: مندوب = light blue AND مستحب = dark blue\n",
    "* obligatory: واجب =  purple AND فرض = rose"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import OrderedDict\n",
    "\n",
    "query = OrderedDict({'forbidden': [], 'discouraged': [], 'neutral': [], 'recommended': [], 'obligatory': []})\n",
    "query['forbidden'].append('محظور')\n",
    "query['forbidden'].append('حرام')\n",
    "query['discouraged'].append('مكروه')\n",
    "query['discouraged'].append('مذموم')\n",
    "query['neutral'].append('مباح')\n",
    "query['recommended'].append('مندوب')\n",
    "query['recommended'].append('مستحب')\n",
    "query['obligatory'].append('واجب')\n",
    "query['obligatory'].append('فرض')\n",
    "query"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python",
   "pygments_lexer": "ipython3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
