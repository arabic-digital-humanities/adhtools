{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import codecs\n",
    "\n",
    "fiqh_file = '/home/jvdzwaan/code/fiqh_corpus/0179MalikIbnAnas.Muwatta.txt'\n",
    "\n",
    "with codecs.open(fiqh_file, encoding='utf-8') as f:\n",
    "    text = f.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "md_header = re.compile(r'#(.+)')\n",
    "result = md_header.finditer(text)\n",
    "print result\n",
    "for m in result:\n",
    "    print m.group()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = re.sub(r'#META#(.+)\\n', '', text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = re.sub(r'######OpenITI#\\n', '', text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fiqh_file = '/home/jvdzwaan/data/tmp/adh/test/data/0274AhmadBarqi.Mahasin.txt'\n",
    "\n",
    "with codecs.open(fiqh_file, encoding='utf-8') as f:\n",
    "    text = f.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "quotes = r'@(.+?)@'\n",
    "pages = r'PageV(\\d{2})P(\\d+)'\n",
    "milestones = r'Milestone\\d+'\n",
    "\n",
    "regex = re.compile(pages)\n",
    "result = regex.finditer(text)\n",
    "for m in result:\n",
    "    print m.group()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clean_file = '/home/jvdzwaan/data/tmp/adh/test/data/0274AhmadBarqi.Mahasin-clean.txt'\n",
    "\n",
    "with codecs.open(clean_file, encoding='utf-8') as f:\n",
    "    lines = f.readlines()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fiqh_file = '/home/jvdzwaan/data/tmp/adh/test/data/0274AhmadBarqi.Mahasin.txt'\n",
    "\n",
    "with codecs.open(fiqh_file, encoding='utf-8') as f:\n",
    "    lines_orig = f.readlines()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print len(lines), len(lines_orig)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk import word_tokenize\n",
    "from Bio import pairwise2\n",
    "import six\n",
    "import re\n",
    "import string\n",
    "\n",
    "\n",
    "def tokenize(text):\n",
    "    tokens = word_tokenize(text)\n",
    "    \n",
    "    # nltk tokenizer replaces \" (double quotes) with `` and ''.\n",
    "    # We want to keep the double quotes, so replace them again.\n",
    "    tokens = ['\"' if t == '``' or t == \"''\" else t for t in tokens]\n",
    "    \n",
    "    return tokens\n",
    "    \n",
    "\n",
    "def get_spaces_pattern(text):\n",
    "    # replace regular expressions special characters\n",
    "    for p in ('(', ')'):\n",
    "            text = text.replace(p, '#')\n",
    "    tokens = tokenize(text)\n",
    "    m = re.match(r'( *)'.join(tokens), text)\n",
    "    return m.groups()\n",
    "\n",
    "def merge_sentences(l1, l2):\n",
    "    if l1 == l2:\n",
    "        print 'same!'\n",
    "        return l1\n",
    "    merged = []\n",
    "\n",
    "    print 'sentence'\n",
    "    idx1 = 0\n",
    "    idx2 = 0\n",
    "    \n",
    "    print type(l1)\n",
    "    print type(l2)\n",
    "    \n",
    "    #print len(l1), len(l2)\n",
    "    # normalize spaces\n",
    "    l1 = re.sub(r' +', u' ', l1)\n",
    "    l2 = re.sub(r' +', u' ', l2)\n",
    "    \n",
    "    # remove trailing whitespace\n",
    "    l1 = l1.strip()\n",
    "    l2 = l2.strip()\n",
    "    \n",
    "    #print len(l1), len(l2)\n",
    "    \n",
    "    tokens1 = tokenize(l1)\n",
    "    tokens2 = tokenize(l2)\n",
    "    \n",
    "    print len(tokens1), len(tokens2)\n",
    "    \n",
    "    print 'l1 is printable: ', all(c in string.printable for c in l1)\n",
    "    print 'l2 is printable: ', all(c in string.printable for c in l2)\n",
    "    \n",
    "    # get spaces patterns\n",
    "    try:\n",
    "        text = l1\n",
    "        spaces1 = get_spaces_pattern(text)\n",
    "    except:\n",
    "        print 'Spaces1 error'\n",
    "        print l1\n",
    "        print u'#'.join(tokens1)\n",
    "        print text\n",
    "        \n",
    "    try:\n",
    "        text = l2\n",
    "        spaces2 = get_spaces_pattern(text)\n",
    "    except:\n",
    "        print 'Spaces2 error'\n",
    "    \n",
    "    print len(spaces1), len(spaces2)\n",
    "    print '---'\n",
    "    \n",
    "    alignment = pairwise2.align.localms(tokens1,tokens2,2,-1,-0.5,-0.1, gap_char=[\"GAP\"])\n",
    "    #print alignment\n",
    "    #print len(alignment)\n",
    "    for t1, t2 in zip(alignment[0][0], alignment[0][1]):\n",
    "        print t1, t2\n",
    "        if t1 == t2:\n",
    "            #print 'equal'\n",
    "            #print_char(1, idx1+1, l1[idx1+1])\n",
    "            merged.append(t1)\n",
    "            try:\n",
    "                merged.append(spaces1[idx1])\n",
    "                if spaces1[idx1] == u' ':\n",
    "                    print 'adding space based on l1'\n",
    "            except IndexError:\n",
    "                # no space to be added\n",
    "                pass\n",
    "            idx1 += 1\n",
    "            idx2 += 1\n",
    "\n",
    "        elif t1 == 'GAP':\n",
    "            merged.append(t2)\n",
    "            try: \n",
    "                merged.append(spaces2[idx2])\n",
    "                if spaces2[idx2] == ' ':\n",
    "                    print 'adding space based on l2'\n",
    "            except IndexError:\n",
    "                pass\n",
    "            idx2 += 1\n",
    "            \n",
    "        elif t2 == 'GAP':\n",
    "            merged.append(t1)\n",
    "            \n",
    "            try:\n",
    "                merged.append(spaces1[idx1])\n",
    "                \n",
    "                if spaces1[idx1] == u' ':\n",
    "                    print 'adding space based on l1'\n",
    "            except IndexError:\n",
    "                pass\n",
    "            idx1 += 1\n",
    "        else:\n",
    "            print 'Problem!'\n",
    "    merged.append('\\n')\n",
    "    return ''.join(merged)\n",
    "\n",
    "nltk.download(\"punkt\")\n",
    "\n",
    "merged = []\n",
    "for l1, l2 in zip(lines[:10], lines_orig[:10]):\n",
    "    print l1\n",
    "    print l2\n",
    "    merged_sentence = merge_sentences(l1, l2)\n",
    "    merged.append(merged_sentence)\n",
    "\n",
    "with codecs.open('/home/jvdzwaan/data/tmp/adh/test/data/0274AhmadBarqi.Mahasin-merged.txt', 'wb', encoding='utf-8') as f:\n",
    "    f.write(''.join(merged))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s1 = lines[1]\n",
    "s2 = lines_orig[1]\n",
    "\n",
    "s1 = s1.strip()\n",
    "s2 = s2.strip()\n",
    "\n",
    "s1 = s1.replace('(', '#')\n",
    "s1 = s1.replace(')', '#')\n",
    "\n",
    "tokens1 = word_tokenize(s1)\n",
    "\n",
    "print s1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print tokens1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "r'( *)'.join(tokens1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "m = re.match(r'( *)'.join(tokens1), s1)\n",
    "print m\n",
    "print m.groups()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens = ['``', \"''\", 'bla']\n",
    "tokens = ['\"' if t == '``' or t == \"''\" else t for t in tokens]\n",
    "print tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s1 = 'This is a \"test\" sentence.'\n",
    "tokens1 = tokenize(s1)\n",
    "print tokens1\n",
    "print r'( *)'.join(tokens1)\n",
    "m = re.match(r'( *)'.join(tokens1), s1)\n",
    "print m.groups()\n",
    "print '\"{}\"'.format(m.group(5))\n",
    "#for match in m:\n",
    "#    print m.group()"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python",
   "pygments_lexer": "ipython3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
