{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# workflow for single book\n",
    "from nlppln import WorkflowGenerator\n",
    "cwl_working_dir = '/home/dafne/cwl-working-dir/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/dafne/anaconda2/envs/adh/lib/python3.6/site-packages/jupyter_client/jsonutil.py:67: DeprecationWarning: Interpreting naive datetime as local 2018-03-27 10:44:14.366817. Please add timezone info to timestamps.\n",
      "  new_obj[k] = extract_dates(v)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Steps\n",
      "  SafarAnalyze............. out_files = wf.SafarAnalyze(cp, in_dir[, analyzer])\n",
      "  SafarStem................ out_files = wf.SafarStem(cp, in_dir[, stemmer])\n",
      "  align.................... changes, metadata = wf.align(file1, file2[, out_dir])\n",
      "  apachetika............... out_files = wf.apachetika(in_files[, tika_server])\n",
      "  basic-text-statistics.... metadata_out = wf.basic_text_statistics(in_files, out_file)\n",
      "  clear-xml-elements....... out_file = wf.clear_xml_elements(element, xml_file)\n",
      "  copy-and-rename.......... copy = wf.copy_and_rename(in_file[, rename])\n",
      "  filter-nes............... filtered_nerstats = wf.filter_nes(nerstats[, name])\n",
      "  flatten-dirs............. out = wf.flatten_dirs(in_dirs)\n",
      "  freqs.................... freqs = wf.freqs(in_files[, name])\n",
      "  frog-dir................. frogout = wf.frog_dir(in_dir[, skip])\n",
      "  frog-single-text......... frogout = wf.frog_single_text(in_file)\n",
      "  frog-to-saf.............. saf = wf.frog_to_saf(in_files)\n",
      "  gather-dirs.............. out = wf.gather_dirs(in_dirs)\n",
      "  ixa-pipe-tok............. out_file = wf.ixa_pipe_tok(language, in_file)\n",
      "  liwc-tokenized........... liwc = wf.liwc_tokenized(in_dir, liwc_dict[, encoding])\n",
      "  lowercase................ out_files = wf.lowercase(in_file)\n",
      "  ls....................... out_files = wf.ls(in_dir[, endswith, recursive])\n",
      "  merge-csv................ merged = wf.merge_csv(in_files[, name])\n",
      "  mkdir.................... out = wf.mkdir(dir_name)\n",
      "  normalize-whitespace-punctuation metadata_out = wf.normalize_whitespace_punctuation(meta_in)\n",
      "  pattern.................. out_files = wf.pattern(in_file[, language])\n",
      "  remove-newlines.......... out_files = wf.remove_newlines(in_file)\n",
      "  remove-xml-elements...... out_file = wf.remove_xml_elements(element, xml_file)\n",
      "  replace-ner.............. out_files = wf.replace_ner(metadata, in_files[, mode])\n",
      "  saf-to-freqs............. freqs = wf.saf_to_freqs(in_files[, mode, name])\n",
      "  saf-to-txt............... out_files = wf.saf_to_txt(in_files)\n",
      "  safar-add-metadata....... out_dir = wf.safar_add_metadata(in_files, meta_in)\n",
      "  save-dir-to-subdir....... out = wf.save_dir_to_subdir(inner_dir, outer_dir)\n",
      "  save-files-to-dir........ out = wf.save_files_to_dir(dir_name, in_files)\n",
      "  save-ner-data............ ner_statistics = wf.save_ner_data(in_files[, name])\n",
      "  tar...................... out = wf.tar(in_file)\n",
      "  textDNA-generate......... json = wf.textDNA_generate(in_dir, mode[, folder_sequences, name_prefix, output_dir])\n",
      "  txt2safar-input.......... metadata, out_dir = wf.txt2safar_input(in_file)\n",
      "  xml-to-text.............. out_files = wf.xml_to_text(in_files[, tag])\n",
      "\n",
      "Workflows\n",
      "  anonymize................ ner_stats, txt = wf.anonymize(txt_dir)\n",
      "  process.................. indexed, safar_output_dir = wf.process(book, cp[, action, analyzer, index_format, index_name])\n",
      "  process_dir.............. indexed, merged_dir, safar_output_dirs = wf.process_dir(cp, in_dir[, action, analyzer, index_format, index_name])\n",
      "  process_list............. indexed, merged_dir, safar_output_dirs = wf.process_list(books, cp[, action, analyzer, index_format, index_name])\n",
      "  safar-workflow........... safar_output_dir = wf.safar_workflow(book, cp[, analyzer])\n"
     ]
    }
   ],
   "source": [
    "with WorkflowGenerator(working_dir=cwl_working_dir) as wf_sub:\n",
    "    wf_sub.load(steps_dir='../adhtools/cwl/')\n",
    "    wf_sub.load(steps_dir='../java/cwl/')\n",
    "    print(wf_sub.list_steps())\n",
    "    \n",
    "    analyzer = wf_sub.add_input(analyzer='string', default='Alkhalil')\n",
    "    book = wf_sub.add_input(book='File')\n",
    "    cp = wf_sub.add_input(cp='string')\n",
    "    \n",
    "    metadata, txt_dir = wf_sub.txt2safar_input(in_file=book)\n",
    "    analyzed_files = wf_sub.SafarAnalyze(in_dir=txt_dir, analyzer=analyzer, cp=cp)\n",
    "    safar_output_dir = wf_sub.safar_add_metadata(in_files=analyzed_files, meta_in=metadata)\n",
    "    \n",
    "    wf_sub.add_outputs(safar_output_dir=safar_output_dir)\n",
    "    \n",
    "    wf_sub.save('../adhtools/cwl/safar-workflow.cwl', wd=True, relative=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/dafne/anaconda2/envs/adh/lib/python3.6/site-packages/jupyter_client/jsonutil.py:67: DeprecationWarning: Interpreting naive datetime as local 2018-03-27 10:47:18.102992. Please add timezone info to timestamps.\n",
      "  new_obj[k] = extract_dates(v)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Steps\n",
      "  SafarAnalyze............. out_files = wf.SafarAnalyze(cp, in_dir[, analyzer])\n",
      "  SafarStem................ out_files = wf.SafarStem(cp, in_dir[, stemmer])\n",
      "  align.................... changes, metadata = wf.align(file1, file2[, out_dir])\n",
      "  apachetika............... out_files = wf.apachetika(in_files[, tika_server])\n",
      "  basic-text-statistics.... metadata_out = wf.basic_text_statistics(in_files, out_file)\n",
      "  blacklabindexer.......... out_dir = wf.blacklabindexer(action, in_dir, index_format, index_name[, contentViewable, textDirection])\n",
      "  clear-xml-elements....... out_file = wf.clear_xml_elements(element, xml_file)\n",
      "  copy-and-rename.......... copy = wf.copy_and_rename(in_file[, rename])\n",
      "  filter-nes............... filtered_nerstats = wf.filter_nes(nerstats[, name])\n",
      "  flatten-dirs............. out = wf.flatten_dirs(in_dirs)\n",
      "  freqs.................... freqs = wf.freqs(in_files[, name])\n",
      "  frog-dir................. frogout = wf.frog_dir(in_dir[, skip])\n",
      "  frog-single-text......... frogout = wf.frog_single_text(in_file)\n",
      "  frog-to-saf.............. saf = wf.frog_to_saf(in_files)\n",
      "  gather-dirs.............. out = wf.gather_dirs(in_dirs)\n",
      "  ixa-pipe-tok............. out_file = wf.ixa_pipe_tok(language, in_file)\n",
      "  liwc-tokenized........... liwc = wf.liwc_tokenized(in_dir, liwc_dict[, encoding])\n",
      "  lowercase................ out_files = wf.lowercase(in_file)\n",
      "  ls....................... out_files = wf.ls(in_dir[, endswith, recursive])\n",
      "  merge-csv................ merged = wf.merge_csv(in_files[, name])\n",
      "  mkdir.................... out = wf.mkdir(dir_name)\n",
      "  normalize-whitespace-punctuation metadata_out = wf.normalize_whitespace_punctuation(meta_in)\n",
      "  pattern.................. out_files = wf.pattern(in_file[, language])\n",
      "  remove-newlines.......... out_files = wf.remove_newlines(in_file)\n",
      "  remove-xml-elements...... out_file = wf.remove_xml_elements(element, xml_file)\n",
      "  replace-ner.............. out_files = wf.replace_ner(metadata, in_files[, mode])\n",
      "  saf-to-freqs............. freqs = wf.saf_to_freqs(in_files[, mode, name])\n",
      "  saf-to-txt............... out_files = wf.saf_to_txt(in_files)\n",
      "  safar-add-metadata....... out_dir = wf.safar_add_metadata(in_files, meta_in)\n",
      "  save-dir-to-subdir....... out = wf.save_dir_to_subdir(inner_dir, outer_dir)\n",
      "  save-files-to-dir........ out = wf.save_files_to_dir(dir_name, in_files)\n",
      "  save-ner-data............ ner_statistics = wf.save_ner_data(in_files[, name])\n",
      "  tar...................... out = wf.tar(in_file)\n",
      "  textDNA-generate......... json = wf.textDNA_generate(in_dir, mode[, folder_sequences, name_prefix, output_dir])\n",
      "  txt2safar-input.......... metadata, out_dir = wf.txt2safar_input(in_file)\n",
      "  xml-to-text.............. out_files = wf.xml_to_text(in_files[, tag])\n",
      "\n",
      "Workflows\n",
      "  anonymize................ ner_stats, txt = wf.anonymize(txt_dir)\n",
      "  process.................. indexed, safar_output_dir = wf.process(book, cp[, action, analyzer, index_format, index_name])\n",
      "  process_dir.............. indexed, merged_dir, safar_output_dirs = wf.process_dir(cp, in_dir[, action, analyzer, index_format, index_name])\n",
      "  process_list............. indexed, merged_dir, safar_output_dirs = wf.process_list(books, cp[, action, analyzer, index_format, index_name])\n",
      "  safar-workflow........... safar_output_dir = wf.safar_workflow(book, cp[, analyzer])\n"
     ]
    }
   ],
   "source": [
    "with WorkflowGenerator(working_dir=cwl_working_dir) as wf:\n",
    "    wf.load(steps_dir='../adhtools/cwl/')\n",
    "    wf.load(steps_dir='../java/cwl/')\n",
    "    wf.load(step_file='https://raw.githubusercontent.com/arabic-digital-humanities/BlackLabIndexer-docker/master/blacklabindexer.cwl')\n",
    "    \n",
    "    print(wf.list_steps())\n",
    "    \n",
    "    analyzer = wf.add_input(analyzer='string', default='Alkhalil')\n",
    "    book = wf.add_input(book='File')\n",
    "    cp = wf.add_input(cp='string')\n",
    "    index_name = wf.add_input(index_name='string', default='corpus')\n",
    "    action = wf.add_input(action='string', default='create')\n",
    "    index_format = wf.add_input(index_format='string', default='safar-analyzer')\n",
    "    text_direction = wf.add_input(text_direction='string', default='rtl')\n",
    "    content_viewable = wf.add_input(content_viewable='boolean', default=True)\n",
    "    \n",
    "    safar_output_dir = wf.safar_workflow(analyzer=analyzer, book=book, cp=cp) #, scatter='book'\n",
    "    indexed = wf.blacklabindexer(action=action, \n",
    "                                 index_format=index_format, \n",
    "                                 index_name=index_name, \n",
    "                                 in_dir=safar_output_dir, \n",
    "                                 text_direction=text_direction, \n",
    "                                 content_viewable=content_viewable)\n",
    "    \n",
    "    wf.add_outputs(indexed=indexed)\n",
    "    \n",
    "    wf.add_outputs(safar_output_dir=safar_output_dir)\n",
    "    \n",
    "    wf.save('../adhtools/cwl/process.cwl', wd=True, relative=False)\n",
    "    \n",
    "    # select txt files -> ls?\n",
    "    # divide and extract metadata -> safar2text_and_metadata\n",
    "    # save to dir -> save_to_dir\n",
    "    # safar analyze -> SafarAnalyze.java -> maak dat ie uit dir leest\n",
    "    # add metadata -> safar_add_metadata\n",
    "    # index with blacklab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/dafne/anaconda2/envs/adh/lib/python3.6/site-packages/jupyter_client/jsonutil.py:67: DeprecationWarning: Interpreting naive datetime as local 2018-03-26 14:56:09.262912. Please add timezone info to timestamps.\n",
      "  new_obj[k] = extract_dates(v)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Steps\n",
      "  SafarAnalyze............. out_files = wf.SafarAnalyze(cp, in_dir[, analyzer])\n",
      "  SafarStem................ out_files = wf.SafarStem(cp, in_dir[, stemmer])\n",
      "  align.................... changes, metadata = wf.align(file1, file2[, out_dir])\n",
      "  apachetika............... out_files = wf.apachetika(in_files[, tika_server])\n",
      "  basic-text-statistics.... metadata_out = wf.basic_text_statistics(in_files, out_file)\n",
      "  blacklabindexer.......... out_dir = wf.blacklabindexer(action, in_dir, index_format, index_name)\n",
      "  clear-xml-elements....... out_file = wf.clear_xml_elements(element, xml_file)\n",
      "  copy-and-rename.......... copy = wf.copy_and_rename(in_file[, rename])\n",
      "  filter-nes............... filtered_nerstats = wf.filter_nes(nerstats[, name])\n",
      "  flatten-dirs............. out = wf.flatten_dirs(in_dirs)\n",
      "  freqs.................... freqs = wf.freqs(in_files[, name])\n",
      "  frog-dir................. frogout = wf.frog_dir(in_dir[, skip])\n",
      "  frog-single-text......... frogout = wf.frog_single_text(in_file)\n",
      "  frog-to-saf.............. saf = wf.frog_to_saf(in_files)\n",
      "  gather-dirs.............. out = wf.gather_dirs(in_dirs)\n",
      "  ixa-pipe-tok............. out_file = wf.ixa_pipe_tok(language, in_file)\n",
      "  liwc-tokenized........... liwc = wf.liwc_tokenized(in_dir, liwc_dict[, encoding])\n",
      "  lowercase................ out_files = wf.lowercase(in_file)\n",
      "  ls....................... out_files = wf.ls(in_dir[, endswith, recursive])\n",
      "  merge-csv................ merged = wf.merge_csv(in_files[, name])\n",
      "  mkdir.................... out = wf.mkdir(dir_name)\n",
      "  normalize-whitespace-punctuation metadata_out = wf.normalize_whitespace_punctuation(meta_in)\n",
      "  pattern.................. out_files = wf.pattern(in_file[, language])\n",
      "  remove-newlines.......... out_files = wf.remove_newlines(in_file)\n",
      "  remove-xml-elements...... out_file = wf.remove_xml_elements(element, xml_file)\n",
      "  replace-ner.............. out_files = wf.replace_ner(metadata, in_files[, mode])\n",
      "  saf-to-freqs............. freqs = wf.saf_to_freqs(in_files[, mode, name])\n",
      "  saf-to-txt............... out_files = wf.saf_to_txt(in_files)\n",
      "  safar-add-metadata....... out_dir = wf.safar_add_metadata(in_files, meta_in)\n",
      "  save-dir-to-subdir....... out = wf.save_dir_to_subdir(inner_dir, outer_dir)\n",
      "  save-files-to-dir........ out = wf.save_files_to_dir(dir_name, in_files)\n",
      "  save-ner-data............ ner_statistics = wf.save_ner_data(in_files[, name])\n",
      "  tar...................... out = wf.tar(in_file)\n",
      "  textDNA-generate......... json = wf.textDNA_generate(in_dir, mode[, folder_sequences, name_prefix, output_dir])\n",
      "  txt2safar-input.......... metadata, out_dir = wf.txt2safar_input(in_file)\n",
      "  xml-to-text.............. out_files = wf.xml_to_text(in_files[, tag])\n",
      "\n",
      "Workflows\n",
      "  anonymize................ ner_stats, txt = wf.anonymize(txt_dir)\n",
      "  process.................. indexed, safar_output_dir = wf.process(book, cp[, action, analyzer, index_format, index_name])\n",
      "  process_dir.............. indexed, merged_dir, safar_output_dirs = wf.process_dir(cp, in_dir[, action, analyzer, index_format, index_name])\n",
      "  process_list............. indexed, merged_dir, safar_output_dirs = wf.process_list(books, cp[, action, analyzer, index_format, index_name])\n",
      "  safar-workflow........... safar_output_dir = wf.safar_workflow(book, cp[, analyzer])\n"
     ]
    }
   ],
   "source": [
    "# Now the scattered version\n",
    "with WorkflowGenerator(working_dir=cwl_working_dir) as wf:\n",
    "    wf.load(steps_dir='../adhtools/cwl/')\n",
    "    wf.load(steps_dir='../java/cwl/')\n",
    "    wf.load(step_file='https://raw.githubusercontent.com/arabic-digital-humanities/BlackLabIndexer-docker/master/blacklabindexer.cwl')\n",
    "    \n",
    "    print(wf.list_steps())\n",
    "    \n",
    "    analyzer = wf.add_input(analyzer='string', default='Alkhalil')\n",
    "    in_dir = wf.add_input(in_dir='Directory')\n",
    "    cp = wf.add_input(cp='string')\n",
    "    index_name = wf.add_input(index_name='string', default='corpus')\n",
    "    action = wf.add_input(action='string', default='create')\n",
    "    index_format = wf.add_input(index_format='string', default='safar-analyzer')\n",
    "    text_direction = wf.add_input(text_direction='string', default='rtl')\n",
    "    content_viewable = wf.add_input(content_viewable='boolean', default=True)\n",
    "    \n",
    "    books = wf.ls(in_dir=in_dir)\n",
    "    safar_output_dirs = wf.safar_workflow(analyzer=analyzer, book=books, cp=cp, scatter='book', scatter_method='dotproduct')\n",
    "    merged_dir = wf.gather_dirs(in_dirs=safar_output_dirs)\n",
    "    indexed = wf.blacklabindexer(action=action, \n",
    "                                 index_format=index_format, \n",
    "                                 index_name=index_name, \n",
    "                                 in_dir=merged_dir, \n",
    "                                 text_direction=text_direction, \n",
    "                                 content_viewable=content_viewable)\n",
    "    wf.add_outputs(indexed=indexed)\n",
    "    \n",
    "    wf.add_outputs(safar_output_dirs=safar_output_dirs)\n",
    "    wf.add_outputs(merged_dir=merged_dir)\n",
    "    \n",
    "    wf.save('../adhtools/cwl/process_dir.cwl', wd=True, relative=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:adh]",
   "language": "python",
   "name": "conda-env-adh-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
