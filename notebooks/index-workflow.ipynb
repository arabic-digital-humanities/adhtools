{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nlppln import WorkflowGenerator\n",
    "#cwl_working_dir = '/home/dafne/cwl-working-dir/'\n",
    "cwl_working_dir = '/home/jvdzwaan/cwl-working-dir/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with WorkflowGenerator(working_dir=cwl_working_dir) as wf_sub:\n",
    "    wf_sub.load(steps_dir='../adhtools/cwl/')\n",
    "    wf_sub.load(steps_dir='../java/cwl/')\n",
    "    print(wf_sub.list_steps())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split and analyze single book\n",
    "with WorkflowGenerator(working_dir=cwl_working_dir) as wf_sub:\n",
    "    wf_sub.load(steps_dir='../adhtools/cwl/')\n",
    "    wf_sub.load(steps_dir='../java/cwl/')\n",
    "    \n",
    "    analyzer = wf_sub.add_input(analyzer='enum', symbols=['Alkhalil', 'BAMA'], default='Alkhalil')\n",
    "    book = wf_sub.add_input(book='File')\n",
    "    cp = wf_sub.add_input(cp='string')\n",
    "    #split_regex_small = wf_sub.add_input(split_regex_small='string', default='Milestone300')\n",
    "    \n",
    "    meta_file, txt_file = wf_sub.extract_metadata(in_file=book) \n",
    "    \n",
    "    snippets = wf_sub.split_text_size(in_file=txt_file)\n",
    "        \n",
    "    analyzed_files = wf_sub.SafarAnalyze(in_files=snippets, analyzer=analyzer, cp=cp)\n",
    "    #merged_file = wf_sub.merge_safar_xml(in_files=analyzed_files)\n",
    "    \n",
    "    #out_file = wf_sub.safar_add_metadata_file(in_file=merged_file, in_file_meta=meta_file)\n",
    "    \n",
    "    # Output is one xml file\n",
    "    wf_sub.add_outputs(out_files=analyzed_files)\n",
    "    \n",
    "    wf_sub.save('../adhtools/cwl/safar-split-and-analyze-file-no-merge.cwl', wd=True, relative=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save xml files\n",
    "with WorkflowGenerator(working_dir=cwl_working_dir) as wf_sub:\n",
    "    wf_sub.load(steps_dir='../adhtools/cwl/')\n",
    "    wf_sub.load(steps_dir='../java/cwl/')\n",
    "    \n",
    "    analyzer = wf_sub.add_input(analyzer='enum', symbols=['Alkhalil', 'BAMA'], default='Alkhalil')\n",
    "    book = wf_sub.add_input(book='File')\n",
    "    cp = wf_sub.add_input(cp='string')\n",
    "    split_regex_small = wf_sub.add_input(split_regex_small='string', default='Milestone300')\n",
    "    \n",
    "    meta_file, txt_file = wf_sub.extract_metadata(in_file=book) \n",
    "    \n",
    "    snippets = wf_sub.split_text(in_file=txt_file, regex=split_regex_small)\n",
    "        \n",
    "    analyzed_files = wf_sub.SafarAnalyze(in_files=snippets, analyzer=analyzer, cp=cp)\n",
    "    \n",
    "    wf_sub.add_outputs(analyzed_files=analyzed_files)\n",
    "    \n",
    "    wf_sub.save('../adhtools/cwl/safar-analyze-file-save-xml.cwl', wd=True, relative=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split and analyze multiple books\n",
    "with WorkflowGenerator(working_dir=cwl_working_dir) as wf:\n",
    "    wf.load(steps_dir='../adhtools/cwl/')\n",
    "    wf.load(steps_dir='../java/cwl/')\n",
    "    \n",
    "    analyzer = wf.add_input(analyzer='enum', symbols=['Alkhalil', 'BAMA'], default='Alkhalil')\n",
    "    in_dir = wf.add_input(in_dir='Directory')\n",
    "    cp = wf.add_input(cp='string')\n",
    "    split_regex_small = wf.add_input(split_regex_small='string', default='Milestone300')   \n",
    "    \n",
    "    books = wf.ls(in_dir=in_dir)\n",
    "    \n",
    "    safar_output_dirs = wf.safar_split_and_analyze_file(analyzer=analyzer, book=books, cp=cp, split_regex_small=split_regex_small,\n",
    "                                                        scatter='book', scatter_method='dotproduct')\n",
    "    \n",
    "    wf.add_outputs(safar_output=safar_output_dirs)\n",
    "    \n",
    "    wf.save('../adhtools/cwl/safar-split-and-analyze-dir.cwl', wd=True, relative=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze a single book\n",
    "with WorkflowGenerator(working_dir=cwl_working_dir) as wf:\n",
    "    wf.load(steps_dir='../adhtools/cwl/')\n",
    "    wf.load(steps_dir='../java/cwl/')\n",
    "    #print(wf_sub.list_steps())\n",
    "    \n",
    "    analyzer = wf.add_input(analyzer='enum', symbols=['Alkhalil', 'BAMA'], default='Alkhalil')\n",
    "    book = wf.add_input(book='File')\n",
    "    cp = wf.add_input(cp='string')\n",
    "    \n",
    "    analyzed_files = wf.SafarAnalyze(in_dir=txt_dir, analyzer=analyzer, cp=cp)\n",
    "    safar_output_dir = wf.safar_add_metadata(in_files=analyzed_files, in_dir_meta=meta_dir, in_file_meta=meta_file)\n",
    "    \n",
    "    # Output is a directory containing xml files. The name of the directory is the name of the book\n",
    "    wf_sub.add_outputs(safar_output_dir=safar_output_dir)\n",
    "    \n",
    "    wf_sub.save('../adhtools/cwl/safar-analyze-file.cwl', wd=True, relative=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scattered version: analyze directory\n",
    "with WorkflowGenerator(working_dir=cwl_working_dir) as wf:\n",
    "    wf.load(steps_dir='../adhtools/cwl/')\n",
    "    wf.load(steps_dir='../java/cwl/')\n",
    "    print(wf.list_steps())\n",
    "    \n",
    "    analyzer = wf.add_input(analyzer='enum', symbols=['Alkhalil', 'BAMA'], default='Alkhalil')\n",
    "    in_dir = wf.add_input(in_dir='Directory')\n",
    "    recursive = wf.add_input(recursive='boolean?', default=False)\n",
    "    cp = wf.add_input(cp='string')\n",
    "    \n",
    "    books = wf.ls(in_dir=in_dir, recursive=recursive)\n",
    "    \n",
    "    out_files = wf.SafarAnalyze(analyzer=analyzer, in_files=books, cp=cp)\n",
    "    \n",
    "    wf.add_outputs(out_files=out_files)\n",
    "    \n",
    "    wf.save('../adhtools/cwl/safar-analyze-dir.cwl', wd=True, relative=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scattered version: analyze directory and save to one directory\n",
    "with WorkflowGenerator(working_dir=cwl_working_dir) as wf:\n",
    "    wf.load(steps_dir='../adhtools/cwl/')\n",
    "    wf.load(steps_dir='../java/cwl/')\n",
    "    print(wf.list_steps())\n",
    "    \n",
    "    analyzer = wf.add_input(analyzer='enum', symbols=['Alkhalil', 'BAMA'], default='Alkhalil')\n",
    "    in_dir = wf.add_input(in_dir='Directory')\n",
    "    cp = wf.add_input(cp='string')\n",
    "    corpus_name = wf.add_input(index_name='string', default='corpus')\n",
    "    \n",
    "    books = wf.ls(in_dir=in_dir)\n",
    "    safar_output_dirs = wf.safar_analyze_book(analyzer=analyzer, book=books, cp=cp, scatter='book', scatter_method='dotproduct')\n",
    "    merged_dir = wf.gather_dirs(in_dirs=safar_output_dirs, dir_name=corpus_name)\n",
    "    \n",
    "    wf.add_outputs(safar_output=merged_dir)\n",
    "    \n",
    "    wf.save('../adhtools/cwl/safar-analyze-corpus.cwl', wd=True, relative=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze input archive and output a single archive\n",
    "# Part of the teamsprint for creating a corpus upload service\n",
    "with WorkflowGenerator(working_dir=cwl_working_dir) as wf:\n",
    "    wf.load(steps_dir='../adhtools/cwl/')\n",
    "    wf.load(steps_dir='../java/cwl/')\n",
    "    print(wf.list_steps())\n",
    "    \n",
    "    wf.set_label('Analyze Arabic texts using SAFAR.')\n",
    "    \n",
    "    analyzer = wf.add_input(analyzer='enum', symbols=['Alkhalil', 'BAMA'], default='Alkhalil', label='Analyzer')\n",
    "    archive = wf.add_input(archive='File', label='Zip file containing texts in OpenITI format')\n",
    "    cp = wf.add_input(cp='string', default='.:/home/jvdzwaan/data/tmp/adh/jars/*:/home/jvdzwaan/code/research-scripts/bin/')\n",
    "    corpus_name = wf.add_input(index_name='string', default='corpus')\n",
    "    \n",
    "    in_dir = wf.archive2dir(archive=archive)\n",
    "    \n",
    "    results_dir = wf.safar_analyze_corpus(cp=cp, in_dir=in_dir, analyzer=analyzer, index_name=corpus_name)\n",
    "    \n",
    "    archive = wf.zip_dir_flat(in_dir=results_dir)\n",
    "  \n",
    "    wf.add_outputs(result=archive)\n",
    "    \n",
    "    wf.save('../adhtools/cwl/safar-analyze-archive.cwl', wd=True, relative=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now the scattered version\n",
    "# Analyze a directory of books and index them\n",
    "with WorkflowGenerator(working_dir=cwl_working_dir) as wf:\n",
    "    wf.load(steps_dir='../adhtools/cwl/')\n",
    "    wf.load(steps_dir='../java/cwl/')\n",
    "    wf.load(step_file='https://raw.githubusercontent.com/arabic-digital-humanities/BlackLabIndexer-docker/master/blacklabindexer.cwl')\n",
    "    \n",
    "    print(wf.list_steps())\n",
    "    \n",
    "    analyzer = wf.add_input(analyzer='enum', symbols=['Alkhalil', 'BAMA'], default='Alkhalil')\n",
    "    in_dir = wf.add_input(in_dir='Directory')\n",
    "    cp = wf.add_input(cp='string')\n",
    "    index_name = wf.add_input(index_name='string', default='corpus')\n",
    "    action = wf.add_input(action='string', default='create')\n",
    "    index_format = wf.add_input(index_format='string', default='safar-analyzer')\n",
    "    text_direction = wf.add_input(text_direction='string', default='rtl')\n",
    "    content_viewable = wf.add_input(content_viewable='boolean', default=True)\n",
    "    xml_dir_name = wf.add_input(xml_dir_name='string', default='xml')\n",
    "    \n",
    "    books = wf.ls(in_dir=in_dir)\n",
    "    safar_output_dirs = wf.safar_analyze_book(analyzer=analyzer, book=books, cp=cp, scatter='book', scatter_method='dotproduct')\n",
    "    merged_dir = wf.gather_dirs(in_dirs=safar_output_dirs, dir_name=xml_dir_name)\n",
    "    indexed = wf.blacklabindexer(action=action, \n",
    "                                 index_format=index_format, \n",
    "                                 index_name=index_name, \n",
    "                                 in_dir=merged_dir, \n",
    "                                 text_direction=text_direction, \n",
    "                                 content_viewable=content_viewable)\n",
    "    # do not use both safar_output_dirs and merged_dir as outputs, because that doesn't work\n",
    "    # (probably because wf.gather_dirs does not copy files, but manipulates symlinks)\n",
    "    wf.add_outputs(indexed=indexed)\n",
    "    wf.add_outputs(merged_dir=merged_dir)\n",
    "    \n",
    "    wf.save('../adhtools/cwl/analyze-and-index-dir.cwl', wd=True, relative=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze directory of texts using different analyzers\n",
    "# DOESN'T WORK ANYMORE SINCE WE UPDATED THE ANALYZER TO AN ENUM, WON'T FIX FOR NOW\n",
    "with WorkflowGenerator(working_dir=cwl_working_dir) as wf:\n",
    "    wf.load(steps_dir='../adhtools/cwl/')\n",
    "    wf.load(steps_dir='../java/cwl/')\n",
    "    wf.load(step_file='https://raw.githubusercontent.com/arabic-digital-humanities/BlackLabIndexer-docker/master/blacklabindexer.cwl')\n",
    "    \n",
    "    print(wf.list_steps())\n",
    "    \n",
    "    analyzers = wf.add_input(analyzer='string[]', default=['Alkhalil', 'BAMA'])\n",
    "    in_dir = wf.add_input(in_dir='Directory')\n",
    "    cp = wf.add_input(cp='string')\n",
    "    index_dir_name = wf.add_input(index_dir_name='string', default='index')\n",
    "    xml_dir_name = wf.add_input(xml_dir_name='string', default='xml')\n",
    "    \n",
    "    indexed, merged_dir = wf.analyze_and_index_dir(cp=cp, \n",
    "                                                   in_dir=in_dir, \n",
    "                                                   analyzer=analyzers, \n",
    "                                                   index_name=analyzers,\n",
    "                                                   xml_dir_name=analyzers,\n",
    "                                                   scatter=['analyzer', 'index_name', 'xml_dir_name'],\n",
    "                                                   scatter_method='dotproduct')\n",
    "    index_dir = wf.gather_dirs(in_dirs=indexed, dir_name=index_dir_name)\n",
    "    xml_dir = wf.gather_dirs(in_dirs=merged_dir, dir_name=xml_dir_name)\n",
    "\n",
    "    # output: a directory containing all indices (one for each analyzer) and \n",
    "    # a directory containing the xml files (contains a subdirectory for each \n",
    "    # analyzer which contains a directory for each book)\n",
    "    wf.add_outputs(indexed=index_dir)\n",
    "    wf.add_outputs(xml=xml_dir)\n",
    "    \n",
    "    wf.save('../adhtools/cwl/analyze-and-index-dir-all-analyzers.cwl', wd=True, relative=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove metadata and divide a file into books/chapters\n",
    "with WorkflowGenerator(working_dir=cwl_working_dir) as wf:\n",
    "    wf.load(steps_dir='../adhtools/cwl/')\n",
    "    wf.load(steps_dir='../java/cwl/')\n",
    "    print(wf.list_steps())\n",
    "    \n",
    "    txt_file = wf.add_input(txt_file='File')\n",
    "    regex = wf.add_input(regex='string[]', default=['### |', '### ||'])\n",
    "    chapter_dir_name = wf.add_input(dir_name='string?')\n",
    "    \n",
    "    chapters = wf.split_text(in_file=txt_file, regex=regex)\n",
    "    chapter_dir = wf.save_files_to_dir(dir_name=chapter_dir_name, in_files=chapters)\n",
    "    \n",
    "    #wf.add_outputs(metadata=meta_file)\n",
    "    wf.add_outputs(chapters=chapter_dir)\n",
    "    \n",
    "    wf.save('../adhtools/cwl/split-books-chapters-file.cwl', wd=True, relative=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# split books/chapters for a directory of text files\n",
    "with WorkflowGenerator(working_dir=cwl_working_dir) as wf:\n",
    "    wf.load(steps_dir='../adhtools/cwl/')\n",
    "    wf.load(steps_dir='../java/cwl/')\n",
    "    print(wf.list_steps())\n",
    "    \n",
    "    in_dir = wf.add_input(in_dir='Directory')\n",
    "    regex = wf.add_input(regex='string[]', default=['### |', '### ||'])\n",
    "    \n",
    "    txt_files = wf.ls(in_dir=in_dir)\n",
    "    chapters = wf.split_books_chapters_file(txt_file=txt_files, regex=regex, \n",
    "                                            scatter='txt_file', scatter_method='dotproduct')\n",
    "    \n",
    "    wf.add_outputs(texts=chapters)\n",
    "    #wf.add_outputs(metadata=metadata)\n",
    "    \n",
    "    wf.save('../adhtools/cwl/split-books-chapters-dir.cwl', wd=True, relative=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split and stem single book (txt file)\n",
    "with WorkflowGenerator(working_dir=cwl_working_dir) as wf:\n",
    "    wf.load(steps_dir='../adhtools/cwl/')\n",
    "    wf.load(steps_dir='../java/cwl/')\n",
    "    \n",
    "    stemmer = wf.add_input(stemmer='enum', \n",
    "                           symbols=['KHOJA', 'LIGHT10', 'ISRI', 'MOTAZ', 'TASHAPHYNE'], \n",
    "                           default='LIGHT10')\n",
    "    txt_file = wf.add_input(txt_file='File')\n",
    "    cp = wf.add_input(cp='string')\n",
    "    split_regex_small = wf.add_input(split_regex_small='string[]', default=['Milestone300'])\n",
    "    \n",
    "    meta_file, txt_file = wf.extract_metadata(in_file=txt_file) \n",
    "    \n",
    "    snippets = wf.split_text(in_file=txt_file, regex=split_regex_small)\n",
    "        \n",
    "    stemmed_files = wf.SafarStem(in_files=snippets, stemmer=stemmer, cp=cp)\n",
    "    merged_file = wf.merge_safar_xml(in_files=stemmed_files)\n",
    "    \n",
    "    #out_file = wf_sub.safar_add_metadata_file(in_file=merged_file, in_file_meta=meta_file)\n",
    "    \n",
    "    # Output is one xml file\n",
    "    #wf_sub.add_outputs(out_file=out_file)\n",
    "    wf.add_outputs(xml_file=merged_file)\n",
    "    \n",
    "    wf.save('../adhtools/cwl/safar-split-and-stem-file.cwl', wd=True, relative=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split and stem a directory of text files\n",
    "# The output is an xml file for each text file in the input directory\n",
    "# Metadata is ignored for now.\n",
    "with WorkflowGenerator(working_dir=cwl_working_dir) as wf:\n",
    "    wf.load(steps_dir='../adhtools/cwl/')\n",
    "    wf.load(steps_dir='../java/cwl/')\n",
    "    \n",
    "    stemmer = wf.add_input(stemmer='enum', \n",
    "                           symbols=['KHOJA', 'LIGHT10', 'ISRI', 'MOTAZ', 'TASHAPHYNE'], \n",
    "                           default='LIGHT10')\n",
    "    in_dir = wf.add_input(in_dir='Directory')\n",
    "    recursive = wf.add_input(recursive='boolean?', default=False)\n",
    "    cp = wf.add_input(cp='string')\n",
    "    split_regex_small = wf.add_input(split_regex_small='string[]', default=['Milestone300'])\n",
    "    \n",
    "    txt_files = wf.ls(in_dir=in_dir, recursive=recursive)\n",
    "    \n",
    "    safar_output_dirs = wf.safar_split_and_stem_file(stemmer=stemmer, txt_file=txt_files, cp=cp, \n",
    "                                                     split_regex_small=split_regex_small,\n",
    "                                                     scatter='txt_file', scatter_method='dotproduct')\n",
    "    \n",
    "    wf.add_outputs(safar_output=safar_output_dirs)\n",
    "    \n",
    "    \n",
    "    wf.save('../adhtools/cwl/safar-split-and-stem-dir.cwl', wd=True, relative=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Stem a single book\n",
    "with WorkflowGenerator(working_dir=cwl_working_dir) as wf_sub:\n",
    "    wf_sub.load(steps_dir='../adhtools/cwl/')\n",
    "    wf_sub.load(steps_dir='../java/cwl/')\n",
    "    print(wf_sub.list_steps())\n",
    "    \n",
    "    \n",
    "    stemmer = wf_sub.add_input(stemmer='enum', \n",
    "                               symbols=['KHOJA', 'LIGHT10', 'ISRI', 'MOTAZ', 'TASHAPHYNE'], \n",
    "                               default='LIGHT10')\n",
    "    book = wf_sub.add_input(book='File')\n",
    "    cp = wf_sub.add_input(cp='string')\n",
    "    \n",
    "    txt_dir, meta_dir, meta_file = wf_sub.txt2safar_input(in_file=book)\n",
    "    stemmed_files = wf_sub.SafarStem(in_dir=txt_dir, stemmer=stemmer, cp=cp)\n",
    "    safar_output_dir = wf_sub.safar_add_metadata(in_files=stemmed_files, in_dir_meta=meta_dir, in_file_meta=meta_file)\n",
    "    \n",
    "    wf_sub.add_outputs(safar_output_dir=safar_output_dir)\n",
    "    \n",
    "    wf_sub.save('../adhtools/cwl/safar-stem-book.cwl', wd=True, relative=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scattered version: stem directory\n",
    "with WorkflowGenerator(working_dir=cwl_working_dir) as wf:\n",
    "    wf.load(steps_dir='../adhtools/cwl/')\n",
    "    wf.load(steps_dir='../java/cwl/')\n",
    "    print(wf.list_steps())\n",
    "    \n",
    "    stemmer = wf.add_input(stemmer='enum', \n",
    "                           symbols=['KHOJA', 'LIGHT10', 'ISRI', 'MOTAZ', 'TASHAPHYNE'], \n",
    "                           default='LIGHT10')\n",
    "    in_dir = wf.add_input(in_dir='Directory')\n",
    "    cp = wf.add_input(cp='string')\n",
    "    books = wf.ls(in_dir=in_dir)\n",
    "    \n",
    "    safar_output_dirs = wf.safar_stem_book(stemmer=stemmer, book=books, cp=cp, scatter='book', scatter_method='dotproduct')\n",
    "    \n",
    "    wf.add_outputs(safar_output=safar_output_dirs)\n",
    "    \n",
    "    wf.save('../adhtools/cwl/safar-stem-dir.cwl', wd=True, relative=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now the scattered version\n",
    "# Stem a directory of books and index them\n",
    "with WorkflowGenerator(working_dir=cwl_working_dir) as wf:\n",
    "    wf.load(steps_dir='../adhtools/cwl/')\n",
    "    wf.load(steps_dir='../java/cwl/')\n",
    "    wf.load(step_file='https://raw.githubusercontent.com/arabic-digital-humanities/BlackLabIndexer-docker/master/blacklabindexer.cwl')\n",
    "    \n",
    "    print(wf.list_steps())\n",
    "    \n",
    "    stemmer = wf.add_input(stemmer='enum', \n",
    "                           symbols=['KHOJA', 'LIGHT10', 'ISRI', 'MOTAZ', 'TASHAPHYNE'], \n",
    "                           default='LIGHT10')\n",
    "    in_dir = wf.add_input(in_dir='Directory')\n",
    "    cp = wf.add_input(cp='string')\n",
    "    index_name = wf.add_input(index_name='string', default='corpus')\n",
    "    action = wf.add_input(action='string', default='create')\n",
    "    index_format = wf.add_input(index_format='string', default='safar-stemmer')\n",
    "    text_direction = wf.add_input(text_direction='string', default='rtl')\n",
    "    content_viewable = wf.add_input(content_viewable='boolean', default=True)\n",
    "    \n",
    "    books = wf.ls(in_dir=in_dir)\n",
    "    safar_output_dirs = wf.safar_stem_book(stemmer=stemmer, book=books, cp=cp, scatter='book', scatter_method='dotproduct')\n",
    "    merged_dir = wf.gather_dirs(in_dirs=safar_output_dirs)\n",
    "    indexed = wf.blacklabindexer(action=action, \n",
    "                                 index_format=index_format, \n",
    "                                 index_name=index_name, \n",
    "                                 in_dir=merged_dir, \n",
    "                                 text_direction=text_direction, \n",
    "                                 content_viewable=content_viewable)\n",
    "    wf.add_outputs(indexed=indexed)\n",
    "    \n",
    "    wf.add_outputs(safar_output_dirs=safar_output_dirs)\n",
    "    wf.add_outputs(merged_dir=merged_dir)\n",
    "    \n",
    "    wf.save('../adhtools/cwl/safar-stem-and-index-dir.cwl', wd=True, relative=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Stem directory of texts using different stemmers\n",
    "# DOESN'T WORK ANYMORE SINCE WE UPDATED THE ANALYZER TO AN ENUM, WON'T FIX FOR NOW\n",
    "with WorkflowGenerator(working_dir=cwl_working_dir) as wf:\n",
    "    wf.load(steps_dir='../adhtools/cwl/')\n",
    "    wf.load(steps_dir='../java/cwl/')\n",
    "    wf.load(step_file='https://raw.githubusercontent.com/arabic-digital-humanities/BlackLabIndexer-docker/master/blacklabindexer.cwl')\n",
    "    \n",
    "    print(wf.list_steps())\n",
    "    \n",
    "    stemmers = wf.add_input(stemmer='string[]', default=['KHOJA', 'LIGHT10', 'ISRI', 'MOTAZ', 'TASHAPHYNE'])\n",
    "    in_dir = wf.add_input(in_dir='Directory')\n",
    "    cp = wf.add_input(cp='string')\n",
    "    \n",
    "    indexed, merged_dir, safar_output_dirs = wf.stem_and_index_dir(cp=cp, \n",
    "                                                                      in_dir=in_dir, \n",
    "                                                                      stemmer=stemmers, \n",
    "                                                                      index_name=stemmers,\n",
    "                                                                      scatter=['stemmer', 'index_name'],\n",
    "                                                                      scatter_method='dotproduct')\n",
    "    wf.add_outputs(indexed=indexed)\n",
    "    \n",
    "    wf.save('../adhtools/cwl/stem-and-index-dir-all-stemmers.cwl', wd=True, relative=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# add metadata to xml files (using the metadata csv)\n",
    "with WorkflowGenerator(working_dir=cwl_working_dir) as wf:\n",
    "    wf.load(steps_dir='../adhtools/cwl/')\n",
    "    wf.load(steps_dir='../java/cwl/')\n",
    "    \n",
    "    in_dir = wf.add_input(in_dir='Directory')\n",
    "    in_file_meta = wf.add_input(in_file_meta='File')\n",
    "    \n",
    "    in_files = wf.ls(in_dir=in_dir)\n",
    "    xml = wf.safar_add_metadata_file(in_file=in_files, in_file_meta=in_file_meta, \n",
    "                                     scatter='in_file', scatter_method='dotproduct')\n",
    "    \n",
    "    wf.add_outputs(xml=xml)\n",
    "    wf.save('../adhtools/cwl/add_metadata_wf.cwl', wd=True, relative=False)\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python",
   "pygments_lexer": "ipython3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
