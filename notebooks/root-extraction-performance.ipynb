{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def stemmer_xml2df(fname):\n",
    "    with codecs.open(fname) as f:\n",
    "        soup = BeautifulSoup(f.read(), 'xml')\n",
    "    \n",
    "    result = []    \n",
    "    for word in soup.find_all('word'):\n",
    "        result.append({'word': word['value'], 'proposed_root': word.analysis['stem']})\n",
    "    \n",
    "    return pd.DataFrame(result)\n",
    "\n",
    "def analyzer_xml2df(fname):\n",
    "    #print(fname)\n",
    "    with codecs.open(fname) as f:\n",
    "        soup = BeautifulSoup(f.read(), 'xml')\n",
    "    \n",
    "    result = []\n",
    "    \n",
    "    for word in soup.find_all('word'):\n",
    "        analyses = word.find_all('analysis')\n",
    "        roots = [a.get('root', 'NO_ROOT') for a in analyses]\n",
    "        roots = list(set(roots))\n",
    "        if len(roots) == 0:\n",
    "            roots.append('NOANALYSIS')\n",
    "        result.append({'word': word['value'], 'proposed_root': '\\\\'.join(roots)})\n",
    "    \n",
    "    #print(len(result))\n",
    "    return pd.DataFrame(result)\n",
    "\n",
    "data = analyzer_xml2df('/home/jvdzwaan/data/tmp/adh/evaluation/alkhalil/0450AbuHasanMawardi.HawiKabir-1000_words/0450AbuHasanMawardi.HawiKabir-sample-000.xml')\n",
    "data.shape\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list(data['word'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from Bio import pairwise2\n",
    "\n",
    "def root_correct(row):\n",
    "    roots = set(row['root'].split('\\\\'))\n",
    "    proposed_roots = set(row['proposed_root'].split('\\\\'))\n",
    "    return len(roots.intersection(proposed_roots)) > 0\n",
    "\n",
    "def compare_to_gs(gs, xml, stemmer=False):\n",
    "    print(gs)\n",
    "    print(xml)\n",
    "    gs = pd.read_csv(gs)\n",
    "    \n",
    "    if stemmer:\n",
    "        data = stemmer_xml2df(xml)\n",
    "    else:\n",
    "        data = analyzer_xml2df(xml)\n",
    "    \n",
    "    print(gs.shape)\n",
    "    print(data.shape)\n",
    "    #print(gs)\n",
    "    #print(list(gs['word']))\n",
    "    #print(len(list(gs['word'])))\n",
    "    #print(list(gs['word'])[0])\n",
    "    if data.shape[0] != gs.shape[0]:\n",
    "        alignments = pairwise2.align.localms(list(gs['word']), list(data['word']),2,-1,-0.5,-0.1, gap_char=[\"MISSING\"], one_alignment_only=True)\n",
    "        l1 = alignments[0][0]\n",
    "        l2 = alignments[0][1]\n",
    "        \n",
    "        result = []\n",
    "        \n",
    "        i1 = 0\n",
    "        i2 = 0\n",
    "        for w1, w2 in zip(l1, l2):\n",
    "            if w1 == w2:\n",
    "                word = gs.loc[i1]['word']\n",
    "                root = gs.loc[i1]['root']\n",
    "                p_root = data.loc[i2]['proposed_root']\n",
    "                \n",
    "                i1 += 1\n",
    "                i2 += 1\n",
    "              \n",
    "            elif w1 == 'MISSING':\n",
    "                word = 'MISSING'\n",
    "                root = 'MISSING'\n",
    "                p_root = data.loc[i2]['proposed_root']\n",
    "                \n",
    "                i2 += 1\n",
    "            elif w2 == 'MISSING':\n",
    "                word = gs.loc[i1]['word']\n",
    "                root = gs.loc[i1]['root']\n",
    "                p_root = 'MISSING'\n",
    "                \n",
    "                i1 += 1\n",
    "            result.append({'word': word, 'root': root, 'proposed_root': p_root})\n",
    "        data = pd.DataFrame(result)\n",
    "    else:\n",
    "        data = pd.concat([gs, data], axis=1, sort=False)\n",
    "        \n",
    "    data['root_correct'] = data.apply(lambda row: root_correct(row), axis=1)\n",
    "        \n",
    "    return data                    \n",
    "\n",
    "gs = '/home/jvdzwaan/data/tmp/adh/evaluation/0450AbuHasanMawardi.HawiKabir-sample.csv'\n",
    "xml = '/home/jvdzwaan/data/tmp/adh/evaluation/alkhalil/0450AbuHasanMawardi.HawiKabir-1000_words/0450AbuHasanMawardi.HawiKabir-sample-000.xml'\n",
    "data = compare_to_gs(gs, xml, stemmer=False)\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data[['root_correct']].apply(lambda x: np.sum(x)/len(x)*100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# after removing words founf by alkhalil, but not in gold standard (2 words)\n",
    "data = data.drop(data[data['word']=='MISSING'].index)\n",
    "data[['root_correct']].apply(lambda x: np.sum(x)/len(x)*100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from nlppln.utils import get_files\n",
    "\n",
    "gs_files = get_files('/home/jvdzwaan/data/tmp/adh/evaluation/gs/')\n",
    "khoja_files = get_files('/home/jvdzwaan/data/tmp/adh/evaluation/khoja/', recursive=True)\n",
    "alkhalil_files = get_files('/home/jvdzwaan/data/tmp/adh/evaluation/alkhalil/', recursive=True)\n",
    "\n",
    "khoja_results = {}\n",
    "for gs, xml in zip(gs_files, khoja_files):\n",
    "    doc_id = os.path.basename(gs)\n",
    "    khoja_results[doc_id] = compare_to_gs(gs, xml, stemmer=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for doc_id, df in khoja_results.items():\n",
    "    print(doc_id, df[['root_correct']].apply(lambda x: np.sum(x)/len(x)*100)[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "alkhalil_results = {}\n",
    "for gs, xml in zip(gs_files, alkhalil_files):\n",
    "    doc_id = os.path.basename(gs)\n",
    "    alkhalil_results[doc_id] = compare_to_gs(gs, xml, stemmer=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for doc_id, df in alkhalil_results.items():\n",
    "    missing = df[df['word']=='MISSING'].index\n",
    "    missing2 = df[df['proposed_root']=='MISSING'].index\n",
    "    df = df.drop(missing)\n",
    "    df = df.drop(missing2)\n",
    "    print(doc_id, df[['root_correct']].apply(lambda x: np.sum(x)/len(x)*100)[0], len(missing), len(missing2))"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python",
   "pygments_lexer": "ipython3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
