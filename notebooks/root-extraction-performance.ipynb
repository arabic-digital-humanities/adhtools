{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import codecs\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "def stemmer_xml2df(fname):\n",
    "    with codecs.open(fname) as f:\n",
    "        soup = BeautifulSoup(f.read(), 'xml')\n",
    "    \n",
    "    result = []    \n",
    "    for word in soup.find_all('word'):\n",
    "        result.append({'word': word['value'], 'proposed_root': word.analysis['stem']})\n",
    "    \n",
    "    return pd.DataFrame(result)\n",
    "\n",
    "def analyzer_xml2df(fname):\n",
    "    #print(fname)\n",
    "    with codecs.open(fname) as f:\n",
    "        soup = BeautifulSoup(f.read(), 'xml')\n",
    "    \n",
    "    result = []\n",
    "    \n",
    "    for word in soup.find_all('word'):\n",
    "        analyses = word.find_all('analysis')\n",
    "        roots = [a.get('root', 'NO_ROOT') for a in analyses]\n",
    "        roots = list(set(roots))\n",
    "        if len(roots) == 0:\n",
    "            roots.append('NOANALYSIS')\n",
    "        result.append({'word': word['value'], 'proposed_root': '\\\\'.join(roots)})\n",
    "    \n",
    "    #print(len(result))\n",
    "    return pd.DataFrame(result)\n",
    "\n",
    "data = analyzer_xml2df('/home/jvdzwaan/data/tmp/adh/evaluation/alkhalil/0450AbuHasanMawardi.HawiKabir-sample.xml')\n",
    "data.shape\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list(data['word'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from Bio import pairwise2\n",
    "\n",
    "def root_correct(row):\n",
    "    roots = set(row['root'].split('\\\\'))\n",
    "    proposed_roots = set(row['proposed_root'].split('\\\\'))\n",
    "    return len(roots.intersection(proposed_roots)) > 0\n",
    "\n",
    "def compare_to_gs(gs, xml, stemmer=False):\n",
    "    print(gs)\n",
    "    print(xml)\n",
    "    gs = pd.read_csv(gs)\n",
    "    \n",
    "    if stemmer:\n",
    "        data = stemmer_xml2df(xml)\n",
    "    else:\n",
    "        data = analyzer_xml2df(xml)\n",
    "    \n",
    "    print(gs.shape)\n",
    "    print(data.shape)\n",
    "    #print(gs)\n",
    "    #print(list(gs['word']))\n",
    "    #print(len(list(gs['word'])))\n",
    "    #print(list(gs['word'])[0])\n",
    "    if data.shape[0] != gs.shape[0]:\n",
    "        alignments = pairwise2.align.localms(list(gs['word']), list(data['word']),2,-1,-0.5,-0.1, gap_char=[\"MISSING\"], one_alignment_only=True)\n",
    "        l1 = alignments[0][0]\n",
    "        l2 = alignments[0][1]\n",
    "        \n",
    "        result = []\n",
    "        \n",
    "        i1 = 0\n",
    "        i2 = 0\n",
    "        for w1, w2 in zip(l1, l2):\n",
    "            if w1 == w2:\n",
    "                word = gs.loc[i1]['word']\n",
    "                root = gs.loc[i1]['root']\n",
    "                p_root = data.loc[i2]['proposed_root']\n",
    "                \n",
    "                i1 += 1\n",
    "                i2 += 1\n",
    "              \n",
    "            elif w1 == 'MISSING':\n",
    "                word = 'MISSING'\n",
    "                root = 'MISSING'\n",
    "                p_root = data.loc[i2]['proposed_root']\n",
    "                \n",
    "                i2 += 1\n",
    "            elif w2 == 'MISSING':\n",
    "                word = gs.loc[i1]['word']\n",
    "                root = gs.loc[i1]['root']\n",
    "                p_root = 'MISSING'\n",
    "                \n",
    "                i1 += 1\n",
    "            result.append({'word': word, 'root': root, 'proposed_root': p_root})\n",
    "        data = pd.DataFrame(result)\n",
    "    else:\n",
    "        data = pd.concat([gs, data], axis=1, sort=False)\n",
    "        data = data.loc[:,~data.columns.duplicated()]\n",
    "        print(data.columns)\n",
    "        \n",
    "    data['root_correct'] = data.apply(lambda row: root_correct(row), axis=1)\n",
    "        \n",
    "    return data                    \n",
    "\n",
    "gs = '/home/jvdzwaan/data/tmp/adh/evaluation/gs/0450AbuHasanMawardi.HawiKabir-sample.csv'\n",
    "xml = '/home/jvdzwaan/data/tmp/adh/evaluation/alkhalil/0450AbuHasanMawardi.HawiKabir-sample.xml'\n",
    "data = compare_to_gs(gs, xml, stemmer=False)\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data[['root_correct']].apply(lambda x: np.sum(x)/len(x)*100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# after removing words founf by alkhalil, but not in gold standard (2 words)\n",
    "data = data.drop(data[data['word']=='MISSING'].index)\n",
    "data[['root_correct']].apply(lambda x: np.sum(x)/len(x)*100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from nlppln.utils import get_files\n",
    "\n",
    "gs_files = get_files('/home/jvdzwaan/data/tmp/adh/evaluation/gs/')\n",
    "khoja_files = get_files('/home/jvdzwaan/data/tmp/adh/evaluation/khoja/', recursive=True)\n",
    "alkhalil_files = get_files('/home/jvdzwaan/data/tmp/adh/evaluation/alkhalil/', recursive=True)\n",
    "\n",
    "khoja_results = {}\n",
    "for gs, xml in zip(gs_files, khoja_files):\n",
    "    doc_id = os.path.basename(gs)\n",
    "    khoja_results[doc_id] = compare_to_gs(gs, xml, stemmer=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_terms(txt_file):\n",
    "    # get the terms list\n",
    "    terms = pd.read_csv(txt_file, encoding='utf-8', index_col=None, header=None)\n",
    "    t = terms[0].tolist()\n",
    "    print('total number of terms:', len(t))\n",
    "    terms = set(t)\n",
    "    print('number of unique terms:', len(terms))\n",
    "    return terms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stopwords = get_terms('/home/jvdzwaan/data/adh/stopwords/custom.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords as sw\n",
    "stopwords_nltk = list(sw.words('arabic'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(stopwords_nltk))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "type(stopwords_nltk)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def is_stopword(row, stopwords):\n",
    "    #print(row['word'])\n",
    "    return row['word'] in stopwords\n",
    " \n",
    "for doc_id, df in khoja_results.items():\n",
    "    #print(df.head())\n",
    "    df['stopword'] = df.apply(lambda row: is_stopword(row, stopwords), axis=1)\n",
    "    df['stopword_nltk'] = df.apply(lambda row: is_stopword(row, stopwords_nltk), axis=1)\n",
    "    print(np.sum(df['stopword']), np.sum(df['stopword_nltk']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for doc_id, df in khoja_results.items():\n",
    "    print(doc_id, df[['root_correct']].apply(lambda x: np.sum(x)/len(x)*100)[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Max' stopwords\n",
    "for doc_id, df in khoja_results.items():\n",
    "    print(doc_id, df.query('stopword == False').shape[0], df.query('stopword == False')[['root_correct']].apply(lambda x: np.sum(x)/len(x)*100)[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# nltk stopwords\n",
    "for doc_id, df in khoja_results.items():\n",
    "    print(doc_id, df.query('stopword_nltk == False').shape[0], df.query('stopword_nltk == False')[['root_correct']].apply(lambda x: np.sum(x)/len(x)*100)[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "alkhalil_results = {}\n",
    "for gs, xml in zip(gs_files, alkhalil_files):\n",
    "    doc_id = os.path.basename(gs)\n",
    "    alkhalil_results[doc_id] = compare_to_gs(gs, xml, stemmer=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_proposed_roots(row):\n",
    "    roots = row['proposed_root'].split('\\\\')\n",
    "    return len(roots)\n",
    "\n",
    "means = []\n",
    "num_single_root = 0\n",
    "total_words = 0\n",
    "\n",
    "for doc_id, df in alkhalil_results.items():\n",
    "    missing = df[df['word']=='MISSING'].index\n",
    "    missing2 = df[df['proposed_root']=='MISSING'].index\n",
    "    df = df.drop(missing)\n",
    "    df = df.drop(missing2)\n",
    "    \n",
    "    df['num_roots'] = df.apply(lambda row: count_proposed_roots(row), axis=1)\n",
    "    print(doc_id, np.mean(df['num_roots']), np.min(df['num_roots']), np.max(df['num_roots']))\n",
    "    means.append(df['num_roots'])\n",
    "    print(df.shape[0])\n",
    "    num_single_root += np.sum(df['num_roots'] == 1)\n",
    "    total_words += df.shape[0]\n",
    "    \n",
    "print(num_single_root)\n",
    "print(total_words)\n",
    "print(np.mean([item for sublist in means for item in sublist]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for doc_id, df in alkhalil_results.items():\n",
    "    missing = df[df['word']=='MISSING'].index\n",
    "    missing2 = df[df['proposed_root']=='MISSING'].index\n",
    "    df = df.drop(missing)\n",
    "    df = df.drop(missing2)\n",
    "    print(doc_id, df[['root_correct']].apply(lambda x: np.sum(x)/len(x)*100)[0], len(missing), len(missing2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for doc_id, df in alkhalil_results.items():\n",
    "    #print(df.head())\n",
    "    df['stopword'] = df.apply(lambda row: is_stopword(row, stopwords), axis=1)\n",
    "    df['stopword_nltk'] = df.apply(lambda row: is_stopword(row, stopwords_nltk), axis=1)\n",
    "    print(np.sum(df['stopword']), np.sum(df['stopword_nltk']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Max' stopwords\n",
    "for doc_id, df in alkhalil_results.items():\n",
    "    missing = df[df['word']=='MISSING'].index\n",
    "    missing2 = df[df['proposed_root']=='MISSING'].index\n",
    "    df = df.drop(missing)\n",
    "    df = df.drop(missing2)\n",
    "    print(doc_id, df.query('stopword == False').shape[0], df.query('stopword == False')[['root_correct']].apply(lambda x: np.sum(x)/len(x)*100)[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# nltk stopwords\n",
    "for doc_id, df in alkhalil_results.items():\n",
    "    missing = df[df['word']=='MISSING'].index\n",
    "    missing2 = df[df['proposed_root']=='MISSING'].index\n",
    "    df = df.drop(missing)\n",
    "    df = df.drop(missing2)\n",
    "    print(doc_id, df.query('stopword_nltk == False').shape[0], df.query('stopword_nltk == False')[['root_correct']].apply(lambda x: np.sum(x)/len(x)*100)[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Does AlKhalil remove all duplicate words?\n",
    "from collections import Counter\n",
    "\n",
    "for doc_id, df in alkhalil_results.items():\n",
    "    c = Counter(list(df['word']))\n",
    "    for w, f in c.most_common(10):\n",
    "        print(w, f)\n",
    "        \n",
    "    print('---')\n",
    "# No, apparantly it doesn't"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# isri stemmer\n",
    "import os\n",
    "from nlppln.utils import get_files\n",
    "\n",
    "isri_files = get_files('/home/jvdzwaan/data/tmp/adh/evaluation/isri/', recursive=True)\n",
    "\n",
    "isri_results = {}\n",
    "for gs, xml in zip(gs_files, isri_files):\n",
    "    doc_id = os.path.basename(gs)\n",
    "    isri_results[doc_id] = compare_to_gs(gs, xml, stemmer=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for doc_id, df in isri_results.items():\n",
    "    missing = df[df['word']=='MISSING'].index\n",
    "    missing2 = df[df['proposed_root']=='MISSING'].index\n",
    "    df = df.drop(missing)\n",
    "    df = df.drop(missing2)\n",
    "    print(doc_id, df.shape[0], df[['root_correct']].apply(lambda x: np.sum(x)/len(x)*100)[0])\n",
    "    print(len(missing), len(missing2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for doc_id, df in isri_results.items():\n",
    "    #print(df.head())\n",
    "    df['stopword'] = df.apply(lambda row: is_stopword(row, stopwords), axis=1)\n",
    "    df['stopword_nltk'] = df.apply(lambda row: is_stopword(row, stopwords_nltk), axis=1)\n",
    "    print(np.sum(df['stopword']), np.sum(df['stopword_nltk']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Max' stopwords\n",
    "for doc_id, df in isri_results.items():\n",
    "    missing = df[df['word']=='MISSING'].index\n",
    "    missing2 = df[df['proposed_root']=='MISSING'].index\n",
    "    df = df.drop(missing)\n",
    "    df = df.drop(missing2)\n",
    "    print(doc_id, df.query('stopword == False').shape[0], df.query('stopword == False')[['root_correct']].apply(lambda x: np.sum(x)/len(x)*100)[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# nltk stopwords\n",
    "for doc_id, df in isri_results.items():\n",
    "    missing = df[df['word']=='MISSING'].index\n",
    "    missing2 = df[df['proposed_root']=='MISSING'].index\n",
    "    df = df.drop(missing)\n",
    "    df = df.drop(missing2)\n",
    "    print(doc_id, df.query('stopword_nltk == False').shape[0], df.query('stopword_nltk == False')[['root_correct']].apply(lambda x: np.sum(x)/len(x)*100)[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_roots(row):\n",
    "    roots = row['root'].split('\\\\')\n",
    "    return len(roots)\n",
    "\n",
    "num_two_roots = 0\n",
    "total_words = 0\n",
    "\n",
    "for gs in gs_files:\n",
    "    gs = pd.read_csv(gs)\n",
    "    gs['num_roots'] = df.apply(lambda row: count_roots(row), axis=1)\n",
    "    print(np.max(gs['num_roots']))\n",
    "    num_two_roots += np.sum(df['num_roots'] == 2)\n",
    "    total_words += df.shape[0]\n",
    "print(num_two_roots)\n",
    "print(total_words)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python",
   "pygments_lexer": "ipython3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
