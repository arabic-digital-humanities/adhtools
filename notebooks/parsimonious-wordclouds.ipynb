{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# one document in the corpus is a book\n",
    "from adhtools.utils import corpus_wordlist\n",
    "from nlppln.utils import get_files\n",
    "\n",
    "#xml_file1 = '/home/jvdzwaan/data/tmp/adh/chapters/1266MuhammadHasanNajafiJawhari.JawahirKalam.xml'\n",
    "#xml_file2 = '/home/jvdzwaan/data/tmp/adh/chapters/0381IbnBabawayh.Hidaya.xml'\n",
    "\n",
    "in_dir = '/home/jvdzwaan/Downloads/2019-02-08-fiqh-newfiles-alkhalil/'\n",
    "        \n",
    "in_files = get_files(in_dir)\n",
    "c = corpus_wordlist(in_files, analyzer=True, field='word')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "from tqdm import tqdm_notebook as tqdm\n",
    "data = [list(terms) for terms in tqdm(c, total=len(in_files))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus_file = 'fiqh-works-alkhalil.pkl'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "import pickle\n",
    "\n",
    "with open(corpus_file, 'wb') as f:\n",
    "    pickle.dump(data, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "import pickle\n",
    "\n",
    "with open(corpus_file, 'rb') as f:\n",
    "    data = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#calculate pwc per book\n",
    "df = calculate(data, w=0.001, top_k=200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# selected works: \n",
    "#0483 Sarakhsī, Mabsūṭ\n",
    "#0450 Māwardī, Ḥāwī\n",
    "#0684 Qarāfi, Dhakhīra\n",
    "#0884 Ibn Mufliḥ, Mubdiʿ\n",
    "#0620 Ibn Qudāma, Mughnī\n",
    "#0460 Ṭūsī, Mabsūṭ\n",
    "#0676 Ḥillī, Sharāʾiʿ\n",
    "\n",
    "import os\n",
    "\n",
    "indices = {}\n",
    "for i, in_file in enumerate(in_files):\n",
    "    \n",
    "    bn = os.path.basename(in_file)\n",
    "    for num in ('0483', '0450', '0684', '0884', '0620', '0460', '0676'):\n",
    "        if bn.startswith(num):\n",
    "            print(i, os.path.splitext(bn)[0])\n",
    "            indices[i] = os.path.splitext(bn)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "del(indices[21])\n",
    "indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# select columns\n",
    "cols1 = ['{}_p'.format(k) for k in indices.keys()]\n",
    "cols2 = ['{}_term'.format(k) for k in indices.keys()]\n",
    "\n",
    "print(cols1)\n",
    "\n",
    "\n",
    "cols = []\n",
    "for c1, c2 in zip(cols1, cols2):\n",
    "    cols.append(c1)\n",
    "    cols.append(c2)\n",
    "print(cols)    \n",
    "df[cols]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# rename columns\n",
    "\n",
    "cols1 = ['{}_p'.format(v) for k, v in indices.items()]\n",
    "cols2 = ['{}_term'.format(v) for k, v in indices.items()]\n",
    "\n",
    "name_cols = []\n",
    "for c1, c2 in zip(cols1, cols2):\n",
    "    name_cols.append(c1)\n",
    "    name_cols.append(c2)\n",
    "print(name_cols) \n",
    "\n",
    "df = df[cols]\n",
    "df.columns = name_cols\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv('pwc-0.001-fiqh-works.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "# one document in the corpus is a school\n",
    "import os\n",
    "import codecs\n",
    "import glob\n",
    "\n",
    "from itertools import chain\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "from nlppln.utils import get_files\n",
    "\n",
    "from adhtools.utils import read_file_analyzer, read_file_stemmer\n",
    "\n",
    "#xml_file1 = '/home/jvdzwaan/data/tmp/adh/chapters/1266MuhammadHasanNajafiJawhari.JawahirKalam.xml'\n",
    "#xml_file2 = '/home/jvdzwaan/data/tmp/adh/chapters/0381IbnBabawayh.Hidaya.xml'\n",
    "\n",
    "md_file = '/home/jvdzwaan/data/adh-corpora/fiqh_corpus/Meta/Metadata_Fiqh.csv'\n",
    "\n",
    "in_dir = '/home/jvdzwaan/data/tmp/adh/2019-02-08-fiqh-newfiles-alkhalil-chapters/'\n",
    "\n",
    "metadata = pd.read_csv(md_file, encoding='utf-8', sep=';|,')\n",
    "#print(metadata.head())\n",
    "schools = metadata.groupby('BookSUBJ')\n",
    "\n",
    "\n",
    "def corpus(schools, in_dir, analyzer=True):\n",
    "\n",
    "    for i, (name, data) in enumerate(schools):\n",
    "        print(i, name)\n",
    "        #print(data['BookURI'])\n",
    "        words = []\n",
    "        #with codecs.open('{}.txt'.format(i), 'w', encoding='utf-8') as f:\n",
    "            \n",
    "        for book in data['BookURI']:\n",
    "            #print(book)\n",
    "            #in_file = os.path.join(in_dir, '{}.xml'.format(book))\n",
    "            in_files = glob.glob('{}/{}*.xml'.format(in_dir, book))\n",
    "            #print(in_files)\n",
    "            for in_file in in_files:\n",
    "                if analyzer:\n",
    "                    ws = read_file_analyzer(in_file)\n",
    "                else:\n",
    "                    ws = read_file_stemmer(in_file)\n",
    "               #print(ws[0])\n",
    "                words.append(ws)\n",
    "        \n",
    "        yield(chain(*words))\n",
    "                #print(len(ws))\n",
    "                #print(ws[0])\n",
    "                #f.write(' '.join(ws))\n",
    "                #f.write('\\n')\n",
    "            \n",
    "c = corpus(schools, in_dir, analyzer=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "# make a sample for tuning the parameter. We use one book per school.\n",
    "import os\n",
    "import codecs\n",
    "import glob\n",
    "\n",
    "from itertools import chain\n",
    "from collections import defaultdict\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "from nlppln.utils import get_files\n",
    "\n",
    "from adhtools.utils import read_file_analyzer, read_file_stemmer\n",
    "\n",
    "#xml_file1 = '/home/jvdzwaan/data/tmp/adh/chapters/1266MuhammadHasanNajafiJawhari.JawahirKalam.xml'\n",
    "#xml_file2 = '/home/jvdzwaan/data/tmp/adh/chapters/0381IbnBabawayh.Hidaya.xml'\n",
    "\n",
    "md_file = '/home/jvdzwaan/data/adh-corpora/fiqh_corpus/Meta/Metadata_Fiqh.csv'\n",
    "\n",
    "in_dir = '/home/jvdzwaan/data/tmp/adh/2019-02-08-fiqh-newfiles-alkhalil-chapters/'\n",
    "\n",
    "metadata = pd.read_csv(md_file, encoding='utf-8')\n",
    "#print(metadata.head())\n",
    "schools = metadata.groupby('BookSUBJ')\n",
    "\n",
    "for i, (name, data) in enumerate(schools):\n",
    "    # we use the oldest book for every school\n",
    "    print(name)\n",
    "    #print(data)\n",
    "    for book in data.iterrows():\n",
    "        # and sample 30.000 words\n",
    "        b = book[1]\n",
    "        #print(book[1])\n",
    "        print(b['BookURI'], b['Number_of_tokens'])\n",
    "        \n",
    "        in_files = glob.glob('{}/{}*.xml'.format(in_dir, book))\n",
    "        #print(in_files)\n",
    "        counts = defaultdict(int)\n",
    "        for in_file in in_files:\n",
    "            if analyzer:\n",
    "                ws = read_file_analyzer(in_file)\n",
    "            else:\n",
    "                ws = read_file_stemmer(in_file)\n",
    "            for word in ws:\n",
    "                counts[word] += 1\n",
    "                \n",
    "                    \n",
    "        \n",
    "        break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "# make a sample for tuning the parameter. We use one book per school.\n",
    "import os\n",
    "import codecs\n",
    "import glob\n",
    "\n",
    "from itertools import chain\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "from nlppln.utils import get_files\n",
    "\n",
    "from adhtools.utils import read_file_analyzer, read_file_stemmer\n",
    "\n",
    "#xml_file1 = '/home/jvdzwaan/data/tmp/adh/chapters/1266MuhammadHasanNajafiJawhari.JawahirKalam.xml'\n",
    "#xml_file2 = '/home/jvdzwaan/data/tmp/adh/chapters/0381IbnBabawayh.Hidaya.xml'\n",
    "\n",
    "md_file = '/home/jvdzwaan/data/adh-corpora/fiqh_corpus/Meta/Metadata_Fiqh.csv'\n",
    "\n",
    "in_dir = '/home/jvdzwaan/data/tmp/adh/2019-02-08-fiqh-newfiles-alkhalil-chapters/'\n",
    "\n",
    "metadata = pd.read_csv(md_file, encoding='utf-8')\n",
    "#print(metadata.head())\n",
    "schools = metadata.groupby('BookSUBJ')\n",
    "\n",
    "\n",
    "def corpus(schools, in_dir, analyzer=True):\n",
    "\n",
    "    for i, (name, data) in enumerate(schools):\n",
    "        print(i, name)\n",
    "        #print(data['BookURI'])\n",
    "        words = []\n",
    "        #with codecs.open('{}.txt'.format(i), 'w', encoding='utf-8') as f:\n",
    "            \n",
    "        for book in data['BookURI']:\n",
    "            #print(book)\n",
    "            #in_file = os.path.join(in_dir, '{}.xml'.format(book))\n",
    "            in_files = glob.glob('{}/{}*.xml'.format(in_dir, book))\n",
    "            #print(in_files)\n",
    "            for in_file in in_files:\n",
    "                if analyzer:\n",
    "                    ws = read_file_analyzer(in_file)\n",
    "                else:\n",
    "                    ws = read_file_stemmer(in_file)\n",
    "               #print(ws[0])\n",
    "                words.append(ws)\n",
    "        \n",
    "        yield(chain(*words))\n",
    "                #print(len(ws))\n",
    "                #print(ws[0])\n",
    "                #f.write(' '.join(ws))\n",
    "                #f.write('\\n')\n",
    "            \n",
    "c = corpus(schools, in_dir, analyzer=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "words = read_file('/home/jvdzwaan/data/tmp/adh/analysis/alkhalil/0311AbuBakrKhallal.WuqufWaTarajjul.xml')\n",
    "print(len(words))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_text_file(in_file):\n",
    "    with codecs.open(in_file, encoding='utf-8') as f:\n",
    "        for ln in f:\n",
    "            for word in ln.split():\n",
    "                #print(word)\n",
    "                yield word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "data = [list(terms) for terms in c]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus_file = 'fiqh-schools-alkhalil.pkl'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "import pickle\n",
    "\n",
    "with open(corpus_file, 'wb') as f:\n",
    "    pickle.dump(data, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "import pickle\n",
    "\n",
    "with open(corpus_file, 'rb') as f:\n",
    "    data = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for terms in data:\n",
    "    print(len(terms))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(data[0][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "from weighwords import ParsimoniousLM\n",
    "\n",
    "model = ParsimoniousLM([terms for terms in data], w=.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "dfs = []\n",
    "\n",
    "top_k = 25\n",
    "for i, terms in enumerate(data):\n",
    "    result = []\n",
    "    for term, p in model.top(top_k, terms, max_iter=100):\n",
    "        result.append({'{}_term'.format(i): term, '{}_p'.format(i): np.exp(p)})\n",
    "        #print(\"    %s %.4f\" % (term, np.exp(p)))\n",
    "        #print(term)\n",
    "    dfs.append(pd.DataFrame(result))\n",
    "    #print('---')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.concat(dfs, axis=1, sort=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from weighwords import ParsimoniousLM\n",
    "\n",
    "def calculate(data, w, top_k=25):\n",
    "    model = ParsimoniousLM([terms for terms in data], w=w)\n",
    "    \n",
    "    # calculate terms and weights\n",
    "    dfs = []\n",
    "\n",
    "    for i, terms in enumerate(data):\n",
    "        result = []\n",
    "        for term, p in model.top(top_k, terms, max_iter=100):\n",
    "            result.append({'{}_term'.format(i): term, '{}_p'.format(i): np.exp(p)})\n",
    "        dfs.append(pd.DataFrame(result))\n",
    "    return pd.concat(dfs, axis=1, sort=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "\n",
    "logger = logging.getLogger(__name__)\n",
    "logging.basicConfig(level=logging.INFO)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "wordcloud_data = []\n",
    "\n",
    "weights = (1.0, 0.5, 0.1, 0.05, 0.01, 0.005, 0.001, 0.0005, 0.0001)\n",
    "\n",
    "for w in weights:\n",
    "    wordcloud_data.append(calculate(data, w=w))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "# pwc6 (selected by Christian) for schools\n",
    "\n",
    "df = calculate(data, w=0.001, top_k=200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "label_0 = 'حنبلي'\n",
    "label_1 = 'حنفي'\n",
    "label_2 = 'شافعي'\n",
    "label_3 = 'شيعي'\n",
    "label_4 = 'مالكي'\n",
    "\n",
    "labels = [label_0, label_1, label_2, label_3, label_4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.columns = ['{}_p'.format(label_0), '{}_term'.format(label_0),\n",
    "              '{}_p'.format(label_1), '{}_term'.format(label_1),\n",
    "              '{}_p'.format(label_2), '{}_term'.format(label_2),\n",
    "              '{}_p'.format(label_3), '{}_term'.format(label_3),\n",
    "              '{}_p'.format(label_4), '{}_term'.format(label_4)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv('pwc-0.001-fiqh-schools.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_terms(txt_file):\n",
    "    # get the terms list\n",
    "    terms = pd.read_csv(txt_file, encoding='utf-8', index_col=None, header=None)\n",
    "    t = terms[0].tolist()\n",
    "    print('total number of terms:', len(t))\n",
    "    terms = set(t)\n",
    "    print('number of unique terms:', len(terms))\n",
    "    return terms\n",
    "stopwords = get_terms('/home/jvdzwaan/data/adh/stopwords/custom.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sw(term):\n",
    "    return 'background-color: yellow' if term in stopwords else ''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(wordcloud_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wordcloud_data[0].style.applymap(sw)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wordcloud_data[1].style.applymap(sw)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wordcloud_data[2].style.applymap(sw)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wordcloud_data[3].style.applymap(sw)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wordcloud_data[4].style.applymap(sw)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wordcloud_data[5].style.applymap(sw)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wordcloud_data[6].style.applymap(sw)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wordcloud_data[7].style.applymap(sw)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wordcloud_data[8].style.applymap(sw)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, w in enumerate(weights):\n",
    "    wordcloud_data[i].style.applymap(sw).to_excel('pwc{}.xls'.format(i), engine='openpyxl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('\\n'.join(list(wordcloud_data[5]['0_term'])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "import codecs\n",
    "\n",
    "c_from_text = [read_text_file(t) for t in ('0.txt', '1.txt', '2.txt', '3.txt', '4.txt')]\n",
    "[len(list(terms)) for terms in c_from_text]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "top_k = 20\n",
    "words = {}\n",
    "\n",
    "for fname, doc in zip(in_files, terms):\n",
    "    print(\"Top %d words in %s:\" % (top_k, os.path.basename(fname)))\n",
    "    words[os.path.basename(fname)] = {}\n",
    "    for term, p in model.top(top_k, doc):\n",
    "        print(\"    %s %.4f\" % (term, np.exp(p)))\n",
    "        words[os.path.basename(fname)][term] = np.exp(p)\n",
    "print(\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Boek dat Christian belangrijk vindt\n",
    "# 0620IbnQudamaMaqdisi.MughniFiFiqh.xml"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(terms))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from wordcloud import WordCloud\n",
    "\n",
    "wc = WordCloud(background_color=\"white\", font_path='/usr/share/fonts/opentype/fonts-hosny-amiri/amiri-quran.ttf')\n",
    "# generate word cloud\n",
    "wc.generate_from_frequencies(words['0179MalikIbnAnas.Muwatta.xml'])\n",
    "\n",
    "# show\n",
    "plt.imshow(wc, interpolation=\"bilinear\")\n",
    "plt.axis(\"off\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wc.generate_from_frequencies(words['0483IbnAhmadSarakhsi.Mabsut.xml'])\n",
    "\n",
    "# show\n",
    "plt.imshow(wc, interpolation=\"bilinear\")\n",
    "plt.axis(\"off\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for word, w in words['0483IbnAhmadSarakhsi.Mabsut.xml'].items():\n",
    "    print(word, w)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python",
   "pygments_lexer": "ipython3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
