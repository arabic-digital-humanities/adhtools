{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from lxml import etree\n",
    "from tqdm import tqdm\n",
    "\n",
    "def stemmer_xml2df2(fname):\n",
    "    result = []\n",
    "    \n",
    "    # Extract the words\n",
    "    context = etree.iterparse(fname, events=('end', ), tag=('word'))\n",
    "    for event, elem in context:\n",
    "        stem = None\n",
    "        for a in elem.getchildren():\n",
    "            if a.tag == 'analysis':\n",
    "                stem = a.attrib['stem']\n",
    "        result.append({'word': elem.attrib['value'], 'proposed_root': stem})\n",
    "        \n",
    "        # make iteration over context fast and consume less memory\n",
    "        #https://www.ibm.com/developerworks/xml/library/x-hiperfparse\n",
    "        elem.clear()\n",
    "        while elem.getprevious() is not None:\n",
    "            del elem.getparent()[0]\n",
    "    \n",
    "    return pd.DataFrame(result)\n",
    "\n",
    "def analyzer_xml2df2(fname):\n",
    "    result = []\n",
    "    \n",
    "    # Extract the words\n",
    "    context = etree.iterparse(fname, events=('end', ), tag=('word'))\n",
    "    for event, elem in context:\n",
    "        word = elem.attrib['value']\n",
    "        #print(repr(word))\n",
    "        if word != '':\n",
    "            roots = []\n",
    "            for a in elem.getchildren():\n",
    "                if a.tag == 'analysis':\n",
    "                    try:\n",
    "                        roots.append(a.attrib['root'])\n",
    "                    except:\n",
    "                        pass\n",
    "            roots = list(set(roots))\n",
    "            if len(roots) == 0:\n",
    "                roots.append('NOANALYSIS')\n",
    "            result.append({'word': elem.attrib['value'], 'proposed_root': '\\\\'.join(roots)})\n",
    "        \n",
    "        # make iteration over context fast and consume less memory\n",
    "        #https://www.ibm.com/developerworks/xml/library/x-hiperfparse\n",
    "        elem.clear()\n",
    "        while elem.getprevious() is not None:\n",
    "            del elem.getparent()[0]\n",
    "    \n",
    "    return pd.DataFrame(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# one document in the corpus is a book\n",
    "from nlppln.utils import get_files\n",
    "\n",
    "#xml_file1 = '/home/jvdzwaan/data/tmp/adh/chapters/1266MuhammadHasanNajafiJawhari.JawahirKalam.xml'\n",
    "#xml_file2 = '/home/jvdzwaan/data/tmp/adh/chapters/0381IbnBabawayh.Hidaya.xml'\n",
    "\n",
    "in_dir = '/home/jvdzwaan/data/tmp/adh/analysis/alkhalil/'\n",
    "\n",
    "def corpus(in_files):\n",
    "    for in_file in in_files:\n",
    "        data = analyzer_xml2df2(in_file)\n",
    "        yield(list(data['word']))\n",
    "        \n",
    "in_files = get_files(in_dir)\n",
    "c = corpus(in_files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "# one document in the corpus is a school\n",
    "import os\n",
    "import codecs\n",
    "\n",
    "from itertools import chain\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "from nlppln.utils import get_files\n",
    "\n",
    "#xml_file1 = '/home/jvdzwaan/data/tmp/adh/chapters/1266MuhammadHasanNajafiJawhari.JawahirKalam.xml'\n",
    "#xml_file2 = '/home/jvdzwaan/data/tmp/adh/chapters/0381IbnBabawayh.Hidaya.xml'\n",
    "\n",
    "md_file = '/home/jvdzwaan/data/adh-corpora/fiqh_corpus/Meta/Metadata_Fiqh.csv'\n",
    "\n",
    "in_dir = '/home/jvdzwaan/data/tmp/adh/analysis/alkhalil/'\n",
    "\n",
    "metadata = pd.read_csv(md_file, encoding='utf-8')\n",
    "#print(metadata.head())\n",
    "schools = metadata.groupby('BookSUBJ')\n",
    "\n",
    "def read_file_analyzer(in_file):\n",
    "    data = analyzer_xml2df2(in_file)\n",
    "    return(list(data['word']))\n",
    "\n",
    "def read_file_stemmer(in_file):\n",
    "    data = stemmer_xml2df2(in_file)\n",
    "    return(list(data['proposed_root']))\n",
    "    \n",
    "\n",
    "def corpus(schools, in_dir, analyzer=True):\n",
    "\n",
    "    for i, (name, data) in enumerate(schools):\n",
    "        print(i, name)\n",
    "        #print(data['BookURI'])\n",
    "        words = []\n",
    "        #with codecs.open('{}.txt'.format(i), 'w', encoding='utf-8') as f:\n",
    "            \n",
    "        for book in data['BookURI']:\n",
    "            #print(book)\n",
    "            in_file = os.path.join(in_dir, '{}.xml'.format(book))\n",
    "            if analyzer:\n",
    "                ws = read_file_analyzer(in_file)\n",
    "            else:\n",
    "                ws = read_file_stemmer(in_file)\n",
    "            #print(ws[0])\n",
    "            words.append(ws)\n",
    "        \n",
    "        yield(chain(*words))\n",
    "                #print(len(ws))\n",
    "                #print(ws[0])\n",
    "                #f.write(' '.join(ws))\n",
    "                #f.write('\\n')\n",
    "            \n",
    "c = corpus(schools, in_dir, analyzer=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "words = read_file('/home/jvdzwaan/data/tmp/adh/analysis/alkhalil/0311AbuBakrKhallal.WuqufWaTarajjul.xml')\n",
    "print(len(words))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_text_file(in_file):\n",
    "    with codecs.open(in_file, encoding='utf-8') as f:\n",
    "        for ln in f:\n",
    "            for word in ln.split():\n",
    "                #print(word)\n",
    "                yield word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "data = [list(terms) for terms in c]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for terms in data:\n",
    "    print(len(terms))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(data[0][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "from weighwords import ParsimoniousLM\n",
    "\n",
    "model = ParsimoniousLM([terms for terms in data], w=.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "dfs = []\n",
    "\n",
    "top_k = 25\n",
    "for i, terms in enumerate(data):\n",
    "    result = []\n",
    "    for term, p in model.top(top_k, terms, max_iter=100):\n",
    "        result.append({'{}_term'.format(i): term, '{}_p'.format(i): np.exp(p)})\n",
    "        #print(\"    %s %.4f\" % (term, np.exp(p)))\n",
    "        #print(term)\n",
    "    dfs.append(pd.DataFrame(result))\n",
    "    #print('---')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.concat(dfs, axis=1, sort=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from weighwords import ParsimoniousLM\n",
    "\n",
    "def calculate(data, w, top_k=25):\n",
    "    model = ParsimoniousLM([terms for terms in data], w=w)\n",
    "    \n",
    "    # calculate terms and weights\n",
    "    dfs = []\n",
    "\n",
    "    for i, terms in enumerate(data):\n",
    "        result = []\n",
    "        for term, p in model.top(top_k, terms, max_iter=10000):\n",
    "            result.append({'{}_term'.format(i): term, '{}_p'.format(i): np.exp(p)})\n",
    "        dfs.append(pd.DataFrame(result))\n",
    "    return pd.concat(dfs, axis=1, sort=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "\n",
    "logger = logging.getLogger(__name__)\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "\n",
    "calculate(data, w=0.005, top_k=25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "wordcloud_data = []\n",
    "\n",
    "for w in (1.0, 0.5, 0.1, 0.05, 0.01, 0.005, 0.001):\n",
    "    wordcloud_data.append(calculate(data, w=w))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_terms(txt_file):\n",
    "    # get the terms list\n",
    "    terms = pd.read_csv(txt_file, encoding='utf-8', index_col=None, header=None)\n",
    "    t = terms[0].tolist()\n",
    "    print('total number of terms:', len(t))\n",
    "    terms = set(t)\n",
    "    print('number of unique terms:', len(terms))\n",
    "    return terms\n",
    "stopwords = get_terms('/home/jvdzwaan/data/adh/stopwords/custom.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sw(term):\n",
    "    return 'background-color: yellow' if term in stopwords else ''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wordcloud_data[6].style.applymap(sw)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('\\n'.join(list(wordcloud_data[5]['0_term'])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "import codecs\n",
    "\n",
    "c_from_text = [read_text_file(t) for t in ('0.txt', '1.txt', '2.txt', '3.txt', '4.txt')]\n",
    "[len(list(terms)) for terms in c_from_text]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "top_k = 20\n",
    "words = {}\n",
    "\n",
    "for fname, doc in zip(in_files, terms):\n",
    "    print(\"Top %d words in %s:\" % (top_k, os.path.basename(fname)))\n",
    "    words[os.path.basename(fname)] = {}\n",
    "    for term, p in model.top(top_k, doc):\n",
    "        print(\"    %s %.4f\" % (term, np.exp(p)))\n",
    "        words[os.path.basename(fname)][term] = np.exp(p)\n",
    "print(\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Boek dat Christian belangrijk vindt\n",
    "# 0620IbnQudamaMaqdisi.MughniFiFiqh.xml"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(terms))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from wordcloud import WordCloud\n",
    "\n",
    "wc = WordCloud(background_color=\"white\", font_path='/usr/share/fonts/opentype/fonts-hosny-amiri/amiri-quran.ttf')\n",
    "# generate word cloud\n",
    "wc.generate_from_frequencies(words['0179MalikIbnAnas.Muwatta.xml'])\n",
    "\n",
    "# show\n",
    "plt.imshow(wc, interpolation=\"bilinear\")\n",
    "plt.axis(\"off\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wc.generate_from_frequencies(words['0483IbnAhmadSarakhsi.Mabsut.xml'])\n",
    "\n",
    "# show\n",
    "plt.imshow(wc, interpolation=\"bilinear\")\n",
    "plt.axis(\"off\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for word, w in words['0483IbnAhmadSarakhsi.Mabsut.xml'].items():\n",
    "    print(word, w)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python",
   "pygments_lexer": "ipython3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
