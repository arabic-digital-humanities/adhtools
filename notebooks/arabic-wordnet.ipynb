{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "# create dict lemma -> root\n",
    "import codecs\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "awn_file = '/home/jvdzwaan/Downloads/arb2-lmf.xml'\n",
    "with codecs.open(awn_file) as f:\n",
    "    soup = BeautifulSoup(f, 'xml')\n",
    "    \n",
    "entries = soup.find_all('LexicalEntry')\n",
    "print(len(entries))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "def is_root(element):\n",
    "    #print element\n",
    "    #print element.name\n",
    "    if element.name == 'WordForm' and element['formType'] == 'root':\n",
    "        return True\n",
    "    return False\n",
    "\n",
    "awn = {}\n",
    "\n",
    "for entry in entries:\n",
    "    lemma = entry.Lemma['writtenForm']\n",
    "    root = entry.find(is_root)\n",
    "    if root:\n",
    "        root = root['writtenForm']\n",
    "        awn[lemma] = root\n",
    "print(len(awn))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "# load analysed words\n",
    "analysed_words = pd.read_csv('/home/jvdzwaan/data/tmp/adh/merged/analysed_words.csv', encoding='utf-8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "types = analysed_words['word'].unique()\n",
    "print(len(types))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "types_in_awn = []\n",
    "for t in types:\n",
    "    if t in awn:\n",
    "        types_in_awn.append(t)\n",
    "print(len(types_in_awn))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "# select word analyses for words for which we can lookup the root in awn\n",
    "awn_analyses = analysed_words.loc[analysed_words['word'].isin(types_in_awn)].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "awn_analyses.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "analyses_per_word = awn_analyses.groupby(['file_name', 'position']).count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "analyses_per_word.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "analysed_words['file_name'].nunique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# the total number of analysed words in the corpus\n",
    "num_words_corpus = analysed_words.groupby(['file_name', 'position']).count().shape[0]\n",
    "print(num_words_corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# the total number of analysed words in the corpus that can be looked up in awn\n",
    "num_words_in_awn = analyses_per_word.shape[0]\n",
    "print(num_words_in_awn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(num_words_in_awn+0.0)/(num_words_corpus+0.0)*100.0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "analyses_per_word['root'].plot.hist(figsize=(15,8), bins=range(analyses_per_word['root'].max()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hoeveel woorden hebben 1 analyse? Van alle woorden in het corpus, ook de niet geanalyseerde\n",
    "single_analysis = np.sum(analyses_per_word['root'] == 1)\n",
    "print('abs {}'.format(single_analysis))\n",
    "print('percentage {}'.format(single_analysis/16899400.0*100.0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# \n",
    "print('abs {}'.format(single_analysis))\n",
    "print('percentage {}'.format(single_analysis/(num_words_corpus+0.0)*100.0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(16899400-num_words_corpus+0.0)/16899400.0*100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "awn_analyses.head(25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "awn_analyses['root_correct'] = awn_analyses.apply(lambda row: row['root']==awn[row['word']], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "awn_analyses.head(25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.sum(awn_analyses['root_correct'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "awn_analyses.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "found_correct = awn_analyses.groupby(['file_name', 'position']).sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "found_correct.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print found_correct.shape\n",
    "np.sum(found_correct['root_correct'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# percentage woorden waarvan de juiste root tussen de voorgestelde roots zit\n",
    "(np.sum(found_correct['root_correct'])+0.0)/found_correct.shape[0]*100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "single_analysis = found_correct[found_correct['root_length'] == 3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# percentage woorden met 1 analyse die ook de juiste is\n",
    "(np.sum(single_analysis['root_correct'])+0.0)/single_analysis.shape[0]*100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import wordnet as wn\n",
    "\n",
    "print sorted(wn.langs())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lemmas = wn.lemmas(u'باب', lang='arb') # door"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for lemma in lemmas:\n",
    "    print lemma.synset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print lemmas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "[l.name() for l in wn.synset('heading.n.01').lemmas()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dog_lemma = wn.lemma(b'dog.n.01.c\\xc3\\xa3o'.decode('utf-8'), lang='por')\n",
    "print dog_lemma.name()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_lemmas = wn.all_lemma_names(pos='n', lang='arb')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print all_lemmas[23]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with codecs.open('/home/jvdzwaan/Downloads/arb2-lmf.xml') as f:\n",
    "    soup = BeautifulSoup(f, 'xml')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print soup.find('LexicalEntry')"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python",
   "pygments_lexer": "ipython3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
