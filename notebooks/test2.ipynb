{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get Max's words\n",
    "#terms_file = '/home/jvdzwaan/data/adh/word-lists/farada.txt'\n",
    "terms_file = '/home/jvdzwaan/data/adh/word-lists/wajaba.txt'\n",
    "terms = pd.read_csv(terms_file, encoding='utf-8', index_col=None, header=None)\n",
    "terms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t = terms[0].tolist()\n",
    "print('total number of terms:', len(t))\n",
    "terms = set(t)\n",
    "print('number of unique terms:', len(terms))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# root farada\n",
    "farada = 'فرض'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# root wajaba\n",
    "wajaba = 'وجب'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from lxml import etree\n",
    "from tqdm import tqdm\n",
    "\n",
    "def stemmer_xml2df2(fname):\n",
    "    result = []\n",
    "    \n",
    "    # Extract the words\n",
    "    context = etree.iterparse(fname, events=('end', ), tag=('word'))\n",
    "    for event, elem in context:\n",
    "        stem = None\n",
    "        for a in elem.getchildren():\n",
    "            if a.tag == 'analysis':\n",
    "                stem = a.attrib['stem']\n",
    "        result.append({'word': elem.attrib['value'], 'proposed_root': stem})\n",
    "        \n",
    "        # make iteration over context fast and consume less memory\n",
    "        #https://www.ibm.com/developerworks/xml/library/x-hiperfparse\n",
    "        elem.clear()\n",
    "        while elem.getprevious() is not None:\n",
    "            del elem.getparent()[0]\n",
    "    \n",
    "    return pd.DataFrame(result)\n",
    "\n",
    "def analyzer_xml2df2(fname):\n",
    "    result = []\n",
    "    \n",
    "    # Extract the words\n",
    "    context = etree.iterparse(fname, events=('end', ), tag=('word'))\n",
    "    for event, elem in tqdm(context):\n",
    "        word = elem.attrib['value']\n",
    "        #print(repr(word))\n",
    "        if word != '':\n",
    "            roots = []\n",
    "            for a in elem.getchildren():\n",
    "                if a.tag == 'analysis':\n",
    "                    try:\n",
    "                        roots.append(a.attrib['root'])\n",
    "                    except:\n",
    "                        pass\n",
    "            roots = list(set(roots))\n",
    "            if len(roots) == 0:\n",
    "                roots.append('NOANALYSIS')\n",
    "            result.append({'word': elem.attrib['value'], 'proposed_root': '\\\\'.join(roots)})\n",
    "        \n",
    "        # make iteration over context fast and consume less memory\n",
    "        #https://www.ibm.com/developerworks/xml/library/x-hiperfparse\n",
    "        elem.clear()\n",
    "        while elem.getprevious() is not None:\n",
    "            del elem.getparent()[0]\n",
    "    \n",
    "    return pd.DataFrame(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "\n",
    "from tabulate import tabulate\n",
    "\n",
    "def print_table(data):\n",
    "    columns = ['# tokens', 'semi-automatic root matches', 'automatically extracted root matches', 'overlap', 'root']\n",
    "    headers = ['name', 'number']\n",
    "    print(tabulate(data[columns].transpose(), headers, tablefmt=\"pipe\"))\n",
    "\n",
    "def root_correct(row, root):\n",
    "    proposed_roots = set(row['proposed_root'].split('\\\\'))\n",
    "    return root in proposed_roots\n",
    "\n",
    "def regex_search(row, regex):\n",
    "    m = regex.search(row['word'])\n",
    "    if m: \n",
    "        return True\n",
    "    return False\n",
    "\n",
    "def get_terms(txt_file):\n",
    "    # get the terms list\n",
    "    terms = pd.read_csv(terms_file, encoding='utf-8', index_col=None, header=None)\n",
    "    t = terms[0].tolist()\n",
    "    print('total number of terms:', len(t))\n",
    "    terms = set(t)\n",
    "    print('number of unique terms:', len(terms))\n",
    "    return terms\n",
    "\n",
    "def match_roots(data, terms, root, document, regex=False):\n",
    "    print('number of tokens: ', data.shape[0])\n",
    "    print('number of terms: ', len(terms))\n",
    "    \n",
    "    if regex:\n",
    "        expr = re.compile(r'({})'.format('|'.join(terms)))\n",
    "        data['root'] = data.apply(lambda row: regex_search(row, expr), axis=1)\n",
    "    else:\n",
    "        # token matching\n",
    "        data['root'] = data.apply(lambda row: row['word'] in terms, axis=1)\n",
    "    data['proposed_root_correct'] = data.apply(lambda row: root_correct(row, root), axis=1)\n",
    "    data['overlap'] = (data['root'] == True) & (data['proposed_root_correct'] == True)\n",
    "    result = {\n",
    "        'root': root,\n",
    "        '# tokens': data.shape[0],\n",
    "        'semi-automatic root matches': data['root'].sum(),\n",
    "        'automatically extracted root matches': data['proposed_root_correct'].sum(),\n",
    "        'overlap': data['overlap'].sum(),\n",
    "        'document': os.path.basename(document)\n",
    "    }\n",
    "    \n",
    "    metadata = pd.DataFrame.from_records([result], index='document')\n",
    "    matches = data[(data['proposed_root_correct'] == True) | (data['root'] == True)]\n",
    "    \n",
    "    return metadata, matches, data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Khoja data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "xml_file = '/home/jvdzwaan/data/tmp/adh/stemmer/0483IbnAhmadSarakhsi.Mabsut.xml'\n",
    "terms_file = '/home/jvdzwaan/data/adh/word-lists/farada-short.txt'\n",
    "\n",
    "terms = get_terms(terms_file)\n",
    "khoja_data = stemmer_xml2df2(xml_file)\n",
    "khoja_orig = khoja_data.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "khoja_orig = khoja_data.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "khoja_orig.to_csv('/home/jvdzwaan/data/adh/0483IbnAhmadSarakhsi.Mabsut-khoja.csv', encoding='utf-8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "khoja_orig = pd.read_csv('/home/jvdzwaan/data/adh/0483IbnAhmadSarakhsi.Mabsut-khoja.csv', encoding='utf-8')\n",
    "khoja_data = khoja_orig.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "terms_file = '/home/jvdzwaan/data/adh/word-lists/farada-short.txt'\n",
    "terms = get_terms(terms_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "khoja_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "khoja_orig.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "document = '/home/jvdzwaan/data/adh/0483IbnAhmadSarakhsi.Mabsut-khoja.csv'\n",
    "khoja_metadata, khoja_matches, khoja_data = match_roots(khoja_data, terms, farada, document, regex=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print_table(khoja_metadata)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "khoja_matches.to_csv('0483IbnAhmadSarakhsi.Mabsut-khoja-farada-short.csv', index=None, encoding='utf-8')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## AlKhalil data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "xml_file = '/home/jvdzwaan/data/tmp/adh/big-xml/0483IbnAhmadSarakhsi.Mabsut.xml'\n",
    "terms_file = '/home/jvdzwaan/data/adh/word-lists/wajaba.txt'\n",
    "\n",
    "terms = get_terms(terms_file)\n",
    "alk_data = analyzer_xml2df2(xml_file)\n",
    "alk_orig = alk_data.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "alk_orig = alk_data.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "alk_orig.to_csv('/home/jvdzwaan/data/adh/0483IbnAhmadSarakhsi.Mabsut-alkhalil.csv', encoding='utf-8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "terms_file = '/home/jvdzwaan/data/adh/word-lists/farada-short.txt'\n",
    "terms = get_terms(terms_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "alk_orig = pd.read_csv('/home/jvdzwaan/data/adh/0483IbnAhmadSarakhsi.Mabsut-alkhalil.csv', encoding='utf-8')\n",
    "alk_data = alk_orig.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "document = '/home/jvdzwaan/data/adh/0483IbnAhmadSarakhsi.Mabsut-alkhalil.csv'\n",
    "alk_metadata, alk_matches, alk_data = match_roots(alk_data, terms, farada, document, regex=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print_table(alk_metadata)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "alk_matches.to_csv('0483IbnAhmadSarakhsi.Mabsut-alkhalil-farada-short.csv', index=None, encoding='utf-8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = alk_orig.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# root wajaba\n",
    "root = 'وجب'\n",
    "data['wajaba'] = data.apply(lambda row: root_correct(row, root), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# root farada\n",
    "root = 'فرض'\n",
    "data['farada'] = data.apply(lambda row: root_correct(row, root), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def num_roots(row):\n",
    "    proposed_roots = set(row['proposed_root'].split('\\\\'))\n",
    "    return len(proposed_roots)\n",
    "\n",
    "data['num_proposed_roots'] = data.apply(lambda row: num_roots(row), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['num_proposed_roots'].mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data[data['wajaba'] == True]['num_proposed_roots'].mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data[data['farada'] == True]['num_proposed_roots'].mean()"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python",
   "pygments_lexer": "ipython3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
